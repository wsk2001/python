# 처음부터 맞춤형 암호화 거래 환경 만들기

출처: https://pylessons.com/RL-BTC-BOT-backbone

## Bitcoin 거래 봇 예제 #1

이 자습서에서는 추가 개발, 테스트 및 자동화된 거래 실험을 수행할 수 있는 사용자 지정 Bitcoin 거래 환경에 대한 단계별 기초를 작성합니다.

이전 **[LunarLander-v2](https://pylessons.com/LunarLander-v2-PPO/)** 및 **[BipedalWalker-v3](https://pylessons.com/BipedalWalker-v3-PPO/)** 자습서에서 저는 자체 RL 환경 개발을 시작하기 위한 배경 지식을 얻기 위해 Proximal Policy Optimization 알고리즘을 작성하는 경험을 수집했습니다. 언급된 튜토리얼은 OpenAI의 체육관 패키지를 기반으로 작성되었으며, 이를 통해 실험할 수 있는 수많은 접근 가능한 환경에서 강화 학습 알고리즘을 실험할 수 있습니다.

이러한 OpenAI 환경은 학습에 적합하지만 결국에는 사용자 지정 문제를 해결하기 위해 에이전트를 설정하려고 합니다. 이렇게 하려면 문제 도메인에 특정한 사용자 지정 환경을 만들어야 합니다. 사용자 지정 환경이 있을 때 암호화 거래를 시뮬레이션하기 위해 사용자 지정 강화 학습 에이전트를 만들 수 있습니다. 이것은 이 자습서의 첫 번째 부분이므로 이 자습서 및 기타 관련 자습서는 내 **[GitHub](https://github.com/pythonlessons/RL-Bitcoin-trading-bot)** 페이지에서 사용할 수 있습니다.

## 환경 소개

먼저 환경이 정확히 무엇인지 알아보자. 환경에는 에이전트를 실행하고 학습을 허용하는 데 필요한 모든 기능이 포함되어 있습니다. 각 환경은 "필수"로 다음 인터페이스를 구현했습니다.

``` py
class CustomEnv:
    def __init__(self, arg1, arg2, ...):
        # Define action space and state size
        
        # Example when using discrete actions of 0,1,2:
        self.action_space = np.array([0, 1, 2])
        
        # Example for using image as input for custom environment:
        self.state_size = np.empty((HEIGHT, WIDTH, CHANNELS)), dtype=np.uint8)
    
    # Execute one time step within the environment
    def step(self, action):
        ...
        
    # Reset the state of the environment to an initial state
    def reset(self):
        ...
    # render environment visualization
    def render(self):
        ...
```

위의 생성자에서 먼저 `action_space`에이전트가 환경에서 수행할 수 있는 모든 작업을 포함하는 의 유형과 모양을 정의합니다. 마찬가지로 `state_size`에이전트가 관찰할 모든 환경 데이터를 포함하는 (위는 이미지를 입력으로 사용한 예) 정의할 것입니다.

우리의 `reset`메서드는 환경을 초기 상태로 재설정하기 위해 주기적으로 호출됩니다. 모델에서 작업을 제공하고 실행해야 하고 다음 관찰이 반환되는 환경을 통해 많은 단계가 이를 따릅니다. 보상이 계산되는 곳이기도 하지만 이에 대해서는 나중에 자세히 설명합니다.

마지막으로 이 `render`메서드는 환경의 표현을 인쇄하기 위해 주기적으로 호출될 수 있습니다. 이것은 인쇄 문만큼 간단할 수도 있고 OpenGL을 사용하여 3D 환경을 렌더링하는 것처럼 복잡할 수도 있습니다. 이 예에서는 print 문으로 시작하지만 나중에 Maplotlib에서 모든 작업을 수행할 계획입니다.

# 비트코인 거래 환경

이 모든 것이 어떻게 작동하는지 보여주기 위해 암호 화폐 거래 환경을 만들 것입니다. 그런 다음 나는 우리 에이전트를 훈련시켜 시장을 이기고 환경 내에서 수익성 있는 거래자가 될 것입니다. 이것은 비트코인이 아니어야 합니다. 우리는 우리가 원하는 시장을 선택할 수 있습니다. 시작하겠습니다! 저는 예시로 비트코인을 선택했습니다.

우리가 가장 먼저 고려해야 할 것은 우리 인간이 어떤 거래를 하고 싶은지 결정하는 방법입니다. 거래를 결정하기 전에 어떤 관찰을 합니까?

일반적으로 전문 트레이더는 몇 가지 기술 지표가 겹쳐진 일부 가격 행동 차트를 볼 가능성이 큽니다. 거기에서 그들은 이 시각적 정보를 유사한 가격 조치에 대한 사전 지식과 결합하여 가격이 어떤 방향으로 움직일 것인지에 대한 정보에 입각한 결정을 내릴 것입니다.

따라서 사용자 지정 에이전트가 가격 동작을 유사하게 이해할 수 있도록 이러한 인간 행동을 코드로 변환해야 합니다. 우리는 state_size가 행동을 취하기 전에 에이전트가 고려해야 할 모든 입력 변수를 포함하기를 원합니다. 이 튜토리얼에서 내 에이전트가 지난 50일 동안의 주요 시장 데이터 포인트(시가, 고가, 저가, 종가 및 일일 거래량)와 해당 계정과 같은 몇 가지 다른 데이터 포인트를 "볼" 수 있기를 바랍니다. 잔액, 현재 오픈 포지션 및 현재 이익.

우리는 각 시간 단계의 에이전트가 현재 가격으로 이어지는 가격 행동과 자신의 포트폴리오 상태를 고려하여 다음 행동에 대한 정보에 입각한 결정을 내리기를 바랍니다. 행동에 대해 말하면, 우리 에이전트는 현재 시간 단계에서 구매, 판매 또는 보류의 세 가지 가능성으로 구성된 action_space를 갖습니다.

그러나 이것은 매번 매수 또는 매도할 비트코인의 양을 아는 것만으로는 충분하지 않습니다. 따라서 우리는 별개의 행동 유형(구매, 판매 및 보류)과 구매/판매 금액의 연속 스펙트럼(각각 계정 잔고/포지션 크기의 0-100%)이 있는 행동 공간을 만들어야 합니다. 이 튜토리얼과 앞으로 나올 몇 가지 튜토리얼에서는 단순함을 위해 금액을 고려하지 않을 것입니다. 그러나 훈련된 에이전트와 작업 환경을 확보한 후 금액을 고려하여 에이전트의 위험을 통합하려고 합니다.

맞춤형 환경을 구현하기 전에 마지막으로 고려해야 할 사항은 보상입니다. 우리는 각 단계에 대해 이전 단계와 현재 단계 간의 계정 잔액 차이를 계산하기 위해 장기 이익을 촉진하고자 합니다. 하지만 아직 작동하는 프로토타입이 없기 때문에 말하기 어렵습니다. 아마도 다른 보상 시스템이 더 잘 작동한다는 것을 알게 될 것입니다. 어쨌든 우리는 에이전트가 지속 불가능한 전략을 사용하여 빠르게 돈을 벌기보다 더 오랫동안 더 높은 균형을 유지하기를 바랍니다.

# 구현

상태 크기, 작업 공간 및 보상을 다루고 정의했으므로 이제 환경을 구현할 차례입니다. 먼저 환경 생성자에서 action_space와 state_size를 정의해야 합니다. 환경에서는 학습할 시장 데이터가 포함된 pandas 데이터 프레임이 전달될 것으로 예상합니다. 여기에 추가하여 데이터 세트 길이, 시작 거래 잔액, 에이전트가 "보기"를 원하는 시장 메모리 단계를 알아야 하며 이러한 모든 매개변수를 우리 `__init__`부분에서 정의합니다. 나는 그것들을 deque 목록으로 설명합니다. 이것은 우리 목록의 크기가 50단계로 제한되어 있음을 의미합니다. 목록에 새 항목을 추가하면 마지막 항목이 제거됩니다. 괜찮아요; 내 **[GitHub 에 시장 이력 데이터를 업로드합니다.](https://github.com/pythonlessons/RL-Bitcoin-trading-bot/tree/main/RL-Bitcoin-trading-bot_1)** 이 튜토리얼 코드와 함께 페이지. 하지만 데이터를 다운로드하고 싶다면 [https://bitcoincharts.com/](https://bitcoincharts.com/charts/bitstampUSD#rg150zigHourlyztgSzm1g10zm2g25zv) 페이지에서 시장 원시 데이터를 다운로드했습니다.

``` py
import pandas as pd
import numpy as np
import random
from collections import deque

class CustomEnv:
    # A custom Bitcoin trading environment
    def __init__(self, df, initial_balance=1000, lookback_window_size=50):
        # Define action space and state size and other custom parameters
        self.df = df.dropna().reset_index()
        self.df_total_steps = len(self.df)-1
        self.initial_balance = initial_balance
        self.lookback_window_size = lookback_window_size

        # Action space from 0 to 3, 0 is hold, 1 is buy, 2 is sell
        self.action_space = np.array([0, 1, 2])

        # Orders history contains the balance, net_worth, crypto_bought, crypto_sold, crypto_held values for the last lookback_window_size steps
        self.orders_history = deque(maxlen=self.lookback_window_size)
        
        # Market history contains the OHCL values for the last lookback_window_size prices
        self.market_history = deque(maxlen=self.lookback_window_size)

        # State size contains Market+Orders history for the last lookback_window_size steps
        self.state_size = (self.lookback_window_size, 10)
```

다음으로, 새로운 환경이 생성될 때마다 또는 기존 환경의 상태를 기본으로 재설정하기 위해 호출되어야 하는 reset 메소드를 작성할 것입니다. 여기에서 시작 균형을 초기 균형으로 설정하고 데이터 세트(훈련 및 테스트 데이터를 분리하는 데 사용)에서 시작 및 종료 단계를 정의하지만 이에 대해서는 나중에 자세히 설명합니다. 보시다시피 주문과 50개 히스토리 단계의 시장 히스토리를 연결하여 기본 상태를 만듭니다.

``` py
# Reset the state of the environment to an initial state
def reset(self, env_steps_size = 0):
    self.balance = self.initial_balance
    self.net_worth = self.initial_balance
    self.prev_net_worth = self.initial_balance
    self.crypto_held = 0
    self.crypto_sold = 0
    self.crypto_bought = 0
    if env_steps_size > 0: # used for training dataset
        self.start_step = random.randint(self.lookback_window_size, self.df_total_steps - env_steps_size)
        self.end_step = self.start_step + env_steps_size
    else: # used for testing dataset
        self.start_step = self.lookback_window_size
        self.end_step = self.df_total_steps

    self.current_step = self.start_step

    for i in reversed(range(self.lookback_window_size)):
        current_step = self.current_step - i
        self.orders_history.append([self.balance, self.net_worth, self.crypto_bought, self.crypto_sold, self.crypto_held])
        self.market_history.append([self.df.loc[current_step, 'Open'],
                                    self.df.loc[current_step, 'High'],
                                    self.df.loc[current_step, 'Low'],
                                    self.df.loc[current_step, 'Close'],
                                    self.df.loc[current_step, 'Volume']
                                    ])

    state = np.concatenate((self.market_history, self.orders_history), axis=1)
    return state
```

다음으로 우리의 환경이 한 걸음 더 나아갈 수 있어야 합니다. 우리 에이전트는 구매, 판매 또는 보류 조치를 선택하고 취하여 보상을 계산하고 각 단계에서 다음 관찰을 반환합니다. 실제 상황에서 일반적으로 우리의 가격은 1시간(이 튜토리얼에서는 1시간을 선택했습니다)이 닫힐 때까지 위아래로 변동합니다. 과거 데이터에서는 이러한 움직임을 볼 수 없으므로 생성해야 합니다. 나는 공개 가격과 닫기 가격 사이에서 임의의 가격을 취함으로써 이것을 수행합니다. 내 총 잔액을 사용하고 있기 때문에 비트코인 금액을 얼마나 사고파는지 쉽게 계산할 수 있고 잔액, crypto_bought, crypto_held, crypto_sold 및 net_worth 매개변수를 나타낼 수 있습니다. 내 에이전트에게). 또한 이전 단계와 현재 단계에서 순자산을 빼서 보상을 계산할 수 있습니다. 그리고 마지막으로,`_next_observation()`.

``` py
# Execute one time step within the environment
def step(self, action):
    self.crypto_bought = 0
    self.crypto_sold = 0
    self.current_step += 1

    # Set the current price to a random price between open and close
    current_price = random.uniform(
        self.df.loc[self.current_step, 'Open'],
        self.df.loc[self.current_step, 'Close'])

    if action == 0: # Hold
        pass

    elif action == 1 and self.balance > 0:
        # Buy with 100% of current balance
        self.crypto_bought = self.balance / current_price
        self.balance -= self.crypto_bought * current_price
        self.crypto_held += self.crypto_bought

    elif action == 2 and self.crypto_held>0:
        # Sell 100% of current crypto held
        self.crypto_sold = self.crypto_held
        self.balance += self.crypto_sold * current_price
        self.crypto_held -= self.crypto_sold

    self.prev_net_worth = self.net_worth
    self.net_worth = self.balance + self.crypto_held * current_price

    self.orders_history.append([self.balance, self.net_worth, self.crypto_bought, self.crypto_sold, self.crypto_held])

    # Calculate reward
    reward = self.net_worth - self.prev_net_worth

    if self.net_worth <= self.initial_balance/2:
        done = True
    else:
        done = False

    obs = self._next_observation()

    return obs, reward, done
```

함수를 단계 함수와 병합할 수도 `_next_observation()`있었지만 코드를 좀 더 단순하게 만들기 위해 분리했습니다. `orders_history`여기에서 나는 역사에서 새로운 단계를 밟고 그것을 단계 함수에서 가져온 목록 과 연결합니다 .

``` py
# Get the data points for the given current_step
def _next_observation(self):
    self.market_history.append([self.df.loc[self.current_step, 'Open'],
                                self.df.loc[self.current_step, 'High'],
                                self.df.loc[self.current_step, 'Low'],
                                self.df.loc[self.current_step, 'Close'],
                                self.df.loc[self.current_step, 'Volume']
                                ])
    obs = np.concatenate((self.market_history, self.orders_history), axis=1)
    return obs
```

일반적으로 에이전트가 학습, 수행 등을 수행하는 방법을 보고자 합니다. 따라서 렌더링 기능을 생성해야 합니다. 단순함을 위해 우리는 현재까지의 환경과 순자산을 렌더링할 것입니다.

``` py
# render environment
def render(self):
    print(f'Step: {self.current_step}, Net Worth: {self.net_worth}')
```

기본 환경 프레임이 거의 완성되었습니다. 이제 이 자습서를 너무 길게 만들지 않기 위해 에이전트 작업을 시뮬레이션하는 함수를 만들지만 AI 에이전트로 이러한 작업을 수행하는 대신 무작위로 수행합니다. 향후 자습서에서는 임의의 에이전트 작업을 훈련된 에이전트와 비교하여 에이전트가 무언가를 학습하고 있는지 확인할 수 있습니다.

``` py
def Random_games(env, train_episodes = 50, training_batch_size=500):
    average_net_worth = 0
    for episode in range(train_episodes):
        state = env.reset(env_steps_size = training_batch_size)

        while True:
            env.render()

            action = np.random.randint(3, size=1)[0]

            state, reward, done = env.step(action)

            if env.current_step == env.end_step:
                average_net_worth += env.net_worth
                print("net_worth:", env.net_worth)
                break

    print("average_net_worth:", average_net_worth/train_episodes)
```

`CustomEnv`데이터 프레임으로 환경을 인스턴스화 하고 `Random_games`에이전트로 테스트할 수 있습니다. 내가 설정 `lookback_window_size=50`하고 이것은 국가에서 개최되는 역사 단계가 될 것입니다. 그런 다음 데이터 프레임을 분리하여 RL 에이전트를 훈련하는 데 사용할 데이터 프레임을 훈련하고 테스트합니다. 또한 평가를 위한 별도의 테스트 및 교육 환경이 필요하므로 이제 재미로 만들겠습니다. 마지막으로 `Random_games`빌드된 환경 중 하나에서 함수를 실행해 보겠습니다.

``` py
df = pd.read_csv('./pricedata.csv')
df = df.sort_values('Date')

lookback_window_size = 50
train_df = df[:-720-lookback_window_size]
test_df = df[-720-lookback_window_size:] # 30 days

train_env = CustomEnv(train_df, lookback_window_size=lookback_window_size)
test_env = CustomEnv(test_df, lookback_window_size=lookback_window_size)

Random_games(train_env, train_episodes = 10, training_batch_size=500)
```

무작위 게임의 10개 에피소드 후에 아래와 같은 유사한 결과를 볼 수 있습니다. 10개의 에피소드 동안 제 평균 순자산은 약 981$였습니다. 즉, 우발적인 에이전트는 10개의 에피소드를 통해 평균 19$를 잃었습니다. 이 임의 에이전트를 실행할 때마다 다른 결과를 받게 됩니다. 긍정적일 수도 있다.

``` py
Step: 1771, Net Worth: 973.1386787365324
Step: 1772, Net Worth: 973.1386787365324
Step: 1773, Net Worth: 973.1386787365324
Step: 1774, Net Worth: 973.1386787365324
Step: 1775, Net Worth: 973.1386787365324
Step: 1776, Net Worth: 973.1386787365324
Step: 1777, Net Worth: 973.1386787365324
net_worth: 973.1386787365324
average_net_worth: 981.7271660925813
>>>
```

# 결론:

물론 이 전체 자습서는 앞으로의 모든 자습서에 대한 소개일 뿐입니다. 이것은 몇 가지 행동, 관찰 및 보상 공간이 포함된 흥미롭고 맞춤화된 강화 학습 환경을 만드는 테스트를 재미로 하기 위한 것입니다. 시장을 이기고 이익을 낼 수 있는 에이전트를 만들려면 훨씬 더 많은 시간과 노력이 필요할 것입니다.

읽어 주셔서 감사합니다! 항상 그렇듯이 이 튜토리얼에서 제공하는 모든 코드는 내 **[GitHub](https://github.com/pythonlessons/RL-Bitcoin-trading-bot)** 페이지에서 찾을 수 있으며 무료로 사용할 수 있습니다! 다음 편에서 뵙겠습니다. 아름다운 가격 차트를 만들어 보겠습니다.



---

# Matplotlib 및 Python #2를 사용하여 우아한 Bitcoin RL 거래 에이전트 차트 시각화

이 부분에서는 Matplotlib 및 Python을 사용하여 RL Bitcoin 거래 봇의 시각화를 렌더링하기 위해 이전 자습서에서 작성된 코드를 확장합니다.

 이 부분에서는 Matplotlib 및 Python을 사용하여 RL Bitcoin 거래 봇을 시각화하기 위해 [이전](https://pylessons.com/RL-BTC-BOT-backbone/) 자습서 에서 작성된 코드를 확장합니다 . [이전](https://pylessons.com/RL-BTC-BOT-backbone/) 자습서를 읽지  않았고 내가 작성한 코드에 익숙하지 않은 경우 이 자습서를 더 읽기 전에 먼저 읽는 것이 좋습니다.

Python 라이브러리 에 익숙 `Matplotlib`하지 않더라도 걱정하지 마십시오. 코드를 단계별로 살펴보고 필요에 따라 사용자 지정 시각화를 만들거나 광산을 수정할 수 있습니다. 모든 것을 이해할 수 없다면 이 튜토리얼 코드는 항상 내 [`GitHub`](https://github.com/pythonlessons/RL-Bitcoin-trading-bot) 저장소에서 사용할 수 있습니다.

계속 진행하기 전에 이 튜토리얼에서 만들 내용에 대한 짧은 미리보기를 보여드리겠습니다.

![img](.\images\1_D8IMow60HONnHyBSXHTB0w.gif)Matplotlib로 생성된 작성자의 gif

의 시작하자! 얼핏 보면 상당히 복잡해 보이지만 실제로는 그렇게 어렵지 않습니다. 이미 작성된 Matplotlib의 도움으로 몇 가지 필수 정보로 각 환경 단계에서 업데이트되는 매개변수 몇 개일 뿐입니다.

# 비트코인 거래 봇 시각화

지난 튜토리얼에서 에이전트의 순자산을 표시하는 명령문을 사용하여 간단한 `render`방법을 작성했습니다. `print`다른 필수 메트릭을 추가할 수도 있었지만 이 튜토리얼에 남겨두었습니다. `utils.py`따라서 필요한 경우 세션의 거래 메트릭을 파일에 저장하기 위해 호출되는 새 메서드 파일에 해당 논리를 작성해 보겠습니다 . 라는 간단한 함수를 만드는 것으로 시작하고 `Write_to_file()`이 함수로 전송되는 모든 내용을 텍스트 파일에 기록합니다.

``` py
import os

def Write_to_file(Date, net_worth, filename='{}.txt'.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"))):
    for i in net_worth: 
        Date += " {}".format(i)
    #print(Date)
    if not os.path.exists('logs'):
        os.makedirs('logs')
    file = open("logs/"+filename, 'a+')
    file.write(Date+"\n")
    file.close()
```

이제 간단히 메인 코드에서 `from utils import Write_to_file`. 저는 이 함수를 단계 함수라고 부르는 것을 선호합니다. 우리가 하고 난 직후에 코드 라인을 `self.orders_history.append(...`삽입 하면 작동합니다. `Write_to_file(Date, self.orders_history[-1])`이 파일에 쓸 메트릭을 더 추가할 수 있지만 내 코드의 첫 번째 문제를 찾는 데 충분했습니다.

내 봇이 구매 작업을 수행할 때 내 잔액은 0이 아니라 0에 가깝습니다. 더 정확하게는 -1.1368683772161603e-13입니다. 그래서 잔액이 0보다 큰지 측정하는 것보다 현재 잔액이 전체 초기 잔액의 1% 이상인지 측정하는 것이 더 낫다고 생각했습니다.

`elif action == 1 and self.balance > self.initial_balance/100`: 하지만 이 튜토리얼은 버그 및 기타 개선 사항에 관한 것이 아닙니다. 새로운 `render`방법을 만들어 보겠습니다. `TradingGraph`아직 작성하지 않은 새 클래스를 활용합니다 . 우리는 다음에 그것을 얻을 것입니다. 여기에 전체 코드를 표시하지 않을 것입니다. my `TradingGraph`:

``` py
class CustomEnv:
    def __init__(self, Render_range = 100):
        self.Render_range = Render_range # render range in visualization
    
    def reset(self):
        self.visualization = TradingGraph(Render_range=self.Render_range) # init visualization
        self.trades = deque(maxlen=self.Render_range) # limited orders memory for visualization
        
    def step(self, action):
        Date = self.df.loc[self.current_step, 'Date'] # for visualization
        High = self.df.loc[self.current_step, 'High'] # for visualization
        Low = self.df.loc[self.current_step, 'Low'] # for visualization
        
        if action == 0: # Hold
            pass
        elif action == 1 and self.balance > self.initial_balance/100:
            self.trades.append({'Date' : Date, 'High' : High, 'Low' : Low, 'total': self.crypto_bought, 'type': "buy"})
        elif action == 2 and self.crypto_held>0:
            self.trades.append({'Date' : Date, 'High' : High, 'Low' : Low, 'total': self.crypto_sold, 'type': "sell"})
        
        Write_to_file(Date, self.orders_history[-1])

    # render environment
    def render(self, visualize = False):
        #print(f'Step: {self.current_step}, Net Worth: {self.net_worth}')
        if visualize:
            Date = self.df.loc[self.current_step, 'Date']
            Open = self.df.loc[self.current_step, 'Open']
            Close = self.df.loc[self.current_step, 'Close']
            High = self.df.loc[self.current_step, 'High']
            Low = self.df.loc[self.current_step, 'Low']
            Volume = self.df.loc[self.current_step, 'Volume']

            # Render the environment to the screen
            self.visualization.render(Date, Open, High, Low, Close, Volume, self.net_worth, self.trades)
```

위의 내용은 이전 튜토리얼 부분의 주요 변경 사항입니다. 보시다시피 `__init__`부품에서 렌더링 범위를 초기화해야 합니다. 이것은 우리가 렌더링하려는 히스토리 막대의 수를 의미합니다.

reset 함수에서 우리는 `TradingGraph`클래스로 새로운 객체를 생성합니다. 그리고 물론, 우리는 거래 목록의 제한된 크기의 데크를 만듭니다. 우리는 이 목록을 사용하여 봇이 수행하는 모든 주문(구매/판매)을 넣어 아름다운 플롯에 그릴 수 있습니다.

단계 함수에서 우리는 가격의 주요 포인트(날짜, 고가, 저가)를 얻고 봇이 어떤 주문을 할 때 이 정보를 사전으로 거래 목록에 넣습니다.

그리고 마지막으로 수정된 기능은 렌더링입니다. 보시다시피 True/False 매개변수로 시각화를 켜고 끌 수 있습니다. 여기서 우리는 필요한 모든(날짜, 시가, 종가, 고가, 저가, 거래량) 매개변수와 호출 `self.visualization.render`기능을 취합니다. 또한 순자산이 어떻게 변하고 봇이 판매 또는 구매 주문을 하는 위치를 보여주고 싶기 때문에 동일한 함수에 `self.net_worth`및 매개변수를 보낼 것입니다.`self.trades`

이제 우리 `TradingGraph`에이전트의 순자산 및 거래와 함께 시장 가격 기록 및 거래량을 렌더링하는 데 필요한 모든 정보가 있습니다. 시각화 렌더링을 시작하겠습니다! 먼저 그래프에 필요한 모든 라이브러리를 가져온 다음 `TradingGraph`, 및 해당 `__init__`메서드를 정의합니다. 여기에서 `matplotlib`그림을 만들고 렌더링할 각 서브플롯을 설정할 것입니다.

여기에서 스타일과 그림 크기를 정의하여 차트의 첫 번째 모양을 만듭니다. 먼저, 그래프에 대한 임시 정보를 저장할 몇 가지 deque 목록을 정의합니다. 또한 열려 있는 경우에 대비하여 모든 플롯을 닫고 있습니다. 이는 두 번째 에피소드 그래프 렌더링을 시작할 때 필요합니다(에이전트가 모든 에피소드를 재설정하고 다시 초기화함).

``` py
import pandas as pd
from collections import deque
import matplotlib.pyplot as plt
from mplfinance.original_flavor import candlestick_ohlc
import matplotlib.dates as mpl_dates
from datetime import datetime
import os
import cv2
import numpy as np

class TradingGraph:
    # A crypto trading visualization using matplotlib made to render custom prices which come in following way:
    # Date, Open, High, Low, Close, Volume, net_worth, trades
    # call render every step
    def __init__(self, Render_range):
        self.Volume = deque(maxlen=Render_range)
        self.net_worth = deque(maxlen=Render_range)
        self.render_data = deque(maxlen=Render_range)
        self.Render_range = Render_range

        # We are using the style ‘ggplot’
        plt.style.use('ggplot')
        # close all plots if there are open
        plt.close('all')
        # figsize attribute allows us to specify the width and height of a figure in unit inches
        self.fig = plt.figure(figsize=(16,8)) 

        # Create top subplot for price axis
        self.ax1 = plt.subplot2grid((6,1), (0,0), rowspan=5, colspan=1)
        
        # Create bottom subplot for volume which shares its x-axis
        self.ax2 = plt.subplot2grid((6,1), (5,0), rowspan=1, colspan=1, sharex=self.ax1)
        
        # Create a new axis for net worth which shares its x-axis with price
        self.ax3 = self.ax1.twinx()

        # Formatting Date
        self.date_format = mpl_dates.DateFormatter('%d-%m-%Y')
        #self.date_format = mpl_dates.DateFormatter('%d-%m-%Y')
        
        # Add paddings to make graph easier to view
        #plt.subplots_adjust(left=0.07, bottom=-0.1, right=0.93, top=0.97, wspace=0, hspace=0)

        # we need to set layers
        self.ax2.set_xlabel('Date')
        self.ax1.set_ylabel('Price')
        self.ax3.set_ylabel('Balance')

        # I use tight_layout to replace plt.subplots_adjust
        self.fig.tight_layout()

        # Show the graph with matplotlib
        plt.show()
```

보시다시피 이 `plt.subplot2grid(...)`방법을 사용하여 먼저 그림 상단에 주요 서브플롯을 생성하여 시장 촛대 데이터를 렌더링한 다음 볼륨에 대해 그 아래에 또 다른 서브플롯을 만듭니다. 마지막으로 `twinx()`동일한 x축을 공유하는 다른 그리드를 맨 위에 오버레이할 수 있는 기능이 있는 세 번째 축을 만듭니다. 함수 의 첫 번째 인수는 `subplot2grid`서브플롯의 크기이고 두 번째 인수는 Figure 내의 위치입니다.

`matplotlib`일반적으로 라이브러리 를 사용하여 일부 그래프를 플로팅합니다 . 대부분 큰 흰색 여백이 있어 그래프가 작아지므로 제거하는 것이 좋습니다. 지금까지는 `subplot_adjust()`함수를 사용했지만 tigh_layout()이 훨씬 낫다는 것을 알았습니다. 흰색 여백 크기를 측정할 필요가 없습니다. 또한 불필요하지만 측면에 레이블이 있는 차트가 훨씬 보기 좋으므로 나중에 잊지 않도록 날짜, 가격, 잔액 축을 설정합니다. 마지막으로 가장 중요한 것은 다음을 사용하여 그림을 화면에 렌더링하는 것입니다 `plt.show()`.

보시다시피 이 `plt.subplot2grid(...)`방법을 사용하여 먼저 그림 상단에 주요 서브플롯을 생성하여 시장 촛대 데이터를 렌더링한 다음 볼륨에 대해 그 아래에 또 다른 서브플롯을 만듭니다. 마지막으로 `twinx()`동일한 x축을 공유하는 다른 그리드를 맨 위에 오버레이할 수 있는 기능이 있는 세 번째 축을 만듭니다. 함수 의 첫 번째 인수는 `subplot2grid`서브플롯의 크기이고 두 번째 인수는 Figure 내의 위치입니다.

`matplotlib`일반적으로 라이브러리 를 사용하여 일부 그래프를 플로팅합니다 . 대부분 큰 흰색 여백이 있어 그래프가 작아지므로 제거하는 것이 좋습니다. 지금까지는 `subplot_adjust()`함수를 사용했지만 tigh_layout()이 훨씬 낫다는 것을 알았습니다. 흰색 여백 크기를 측정할 필요가 없습니다. 또한 불필요하지만 측면에 레이블이 있는 차트가 훨씬 보기 좋으므로 나중에 잊지 않도록 날짜, 가격, 잔액 축을 설정합니다. 마지막으로 가장 중요한 것은 다음을 사용하여 그림을 화면에 렌더링하는 것입니다 `plt.show()`.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-visualization/Figure_1.png)Matplotlib로 생성된 작성자의 이미지

다음으로 `render`메서드를 작성해 보겠습니다. 이것은 현재 시간 단계의 모든 정보를 가져와 화면에 실시간 표현을 렌더링합니다. 가장 복잡한 것 중 하나는 가격 그래프를 렌더링하는 것입니다. 그러나 운 좋게도 일을 단순하게 유지하기 위해 `mplfinance`이전에 가져온 라이브러리의 OHCL 막대를 사용할 수 있습니다. `pip install mplfinance`이 패키지가 촛대 그래프를 그리는 가장 쉬운 방법 이므로 아직 가지고 있지 않은 경우 를 작성 하십시오.

이 `date2num`함수는 OHCL 렌더링 프로세스에 필요한 날짜 형식을 타임스탬프로 다시 지정하는 데 사용됩니다. 모든 `render`단계에서 필요한 OHCL 데이터를 메모리를 제거하고 플롯에 렌더링됩니다. 그래프는 동적이므로 새 프레임을 생성하기 전에 이전 프레임을 지워야 합니다. 그렇게 한 후 OHCL 데이터를 가져와 촛대 그래프를 `self.ax1`서브플롯에 렌더링합니다.

매 프레임마다 모든 축을 지우기 때문에 레이블도 지워지므로 레이블을 `__init__`부품에서 렌더링 부품으로 이동합니다. 마지막으로 가장 중요한 것은 을 사용하여 그림을 화면에 렌더링하는 것입니다 `plt.show(block=False)`. block=False를 전달하는 것을 잊으면 렌더링된 첫 번째 단계만 볼 수 있으며 그 이후에는 에이전트가 계속되지 않도록 차단됩니다. 전화하는 것이 중요합니다 `plt.pause()`. `render`그렇지 않으면 각 프레임은 마지막 프레임이 실제로 화면에 표시되기 전에 다음 호출로 지워집니다 .

```python
class TradingGraph:
    def __init__(self, Render_range):
        ...
    
    # Render the environment to the screen
    def render(self, Date, Open, High, Low, Close, Volume, net_worth, trades):
        # before appending to deque list, need to convert Date to special format
        Date = mpl_dates.date2num([pd.to_datetime(Date)])[0]
        self.render_data.append([Date, Open, High, Low, Close])
        
        # Clear the frame rendered last step
        self.ax1.clear()
        candlestick_ohlc(self.ax1, self.render_data, width=0.8/24, colorup='green', colordown='red', alpha=0.8)

        # we need to set layers every step, because we are clearing subplots every step
        self.ax2.set_xlabel('Date')
        self.ax1.set_ylabel('Price')
        self.ax3.set_ylabel('Balance')

        """Display image with matplotlib - interrupting other tasks"""
        # Show the graph without blocking the rest of the program
        plt.show(block=False)
        # Necessary to view frames before they are unrendered
        plt.pause(0.001)
```

위의 코드를 구현한 후 화면에 아름다운 OHCL 막대가 표시되어야 합니다.

.

![img](.\images\gameplay_2.gif)Matplotlib로 생성된 작성자의 gif

따라서 가장 어려운 부분은 이미 완료되었습니다. 이제 볼륨 및 순 가치와 아름답게 형식이 지정된 날짜를 플롯에 추가하고 싶습니다. OHCL 데이터와 마찬가지로 볼륨 및 순 가치 기록 데이터를 deque 목록에 추가하여 수집해야 합니다. 유사하게, 우리는 지우고 서브플롯을 작성 `ax2`하고 `ax3`모든 날짜를 하나의 목록에 넣고 `ax2`서브플롯을 역사적 볼륨으로 채웁니다. 가장 중요한 줄은 `self.ax1.xaxis.set_major_formatter(self.date_format)`모든 `self.fig.autofmt_xdate(),`아름다운 날짜 형식이 수행되는 위치입니다. 순자산을 추가하는 것은 가장 간단한 부분 중 하나이므로 다음 줄을 추가합니다 `self.ax3.plot(Date_Render_range, self.net_worth, color=”blue”)`. 여기까지의 코드는 다음과 같습니다.

``` py
class TradingGraph:
    def __init__(self, Render_range):
        ...
    
    # Render the environment to the screen
    def render(self, Date, Open, High, Low, Close, Volume, net_worth, trades):
        # append volume and net_worth to deque list
        self.Volume.append(Volume)
        self.net_worth.append(net_worth)

        # before appending to deque list, need to convert Date to special format
        Date = mpl_dates.date2num([pd.to_datetime(Date)])[0]
        self.render_data.append([Date, Open, High, Low, Close])
        
        # Clear the frame rendered last step
        self.ax1.clear()
        candlestick_ohlc(self.ax1, self.render_data, width=0.8/24, colorup='green', colordown='red', alpha=0.8)

        # Put all dates to one list and fill ax2 sublot with volume
        Date_Render_range = [i[0] for i in self.render_data]
        self.ax2.clear()
        self.ax2.fill_between(Date_Render_range, self.Volume, 0)

        # draw our net_worth graph on ax3 (shared with ax1) subplot
        self.ax3.clear()
        self.ax3.plot(Date_Render_range, self.net_worth, color="blue")
        
        # beautify the x-labels (Our Date format)
        self.ax1.xaxis.set_major_formatter(self.date_format)
        self.fig.autofmt_xdate()

        # we need to set layers every step, because we are clearing subplots every step
        self.ax2.set_xlabel('Date')
        self.ax1.set_ylabel('Price')
        self.ax3.set_ylabel('Balance')
        
        # I use tight_layout to replace plt.subplots_adjust
        self.fig.tight_layout()

        """Display image with matplotlib - interrupting other tasks"""
        # Show the graph without blocking the rest of the program
        plt.show(block=False)
        # Necessary to view frames before they are unrendered
        plt.pause(0.001)
```

보시다시피 이제 차트는 매우 유익하고 개선할 수 있는 방법이 많이 있지만 이 시점에서 우리는 대부분 주문이 이루어진 위치를 알고 싶으므로 몇 가지 화살표 포인트를 추가해야 합니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-visualization/gameplay_3.gif)

플롯에 아름다운 빨강 및 녹색 화살표를 추가하는 방법을 찾는 데 시간이 걸렸습니다. 그러나 마지막으로 선 플롯의 가까운 사촌인 일반적으로 사용되는 플롯 유형 "산점도"를 찾았습니다. 점을 선분으로 연결하는 대신 점, 원 또는 기타 모양으로 점을 개별적으로 표시합니다. `self.trades`이것은 우리가 메인 프로그램에서 사전을 사용할 곳 입니다. 이 장소는 플롯 기록에서 주문이 이루어진 날짜 위치를 찾기 위해 for 루프를 사용하기 때문에 조금 느릴 수 있습니다. 주문 종류에 따라 초록색 화살표나 빨간색 화살표를 넣었는데 지금은 코드가 상당히 단순해 보이지만 이 부분이 가장 시간이 많이 걸린 것 같아요.

```python
class TradingGraph:
    def __init__(self, Render_range):
        ...
        
    # Render the environment to the screen
    def render(self, Date, Open, High, Low, Close, Volume, net_worth, trades):
        # append volume and net_worth to deque list
        self.Volume.append(Volume)
        self.net_worth.append(net_worth)

        # before appending to deque list, need to convert Date to special format
        Date = mpl_dates.date2num([pd.to_datetime(Date)])[0]
        self.render_data.append([Date, Open, High, Low, Close])
        
        # Clear the frame rendered last step
        self.ax1.clear()
        candlestick_ohlc(self.ax1, self.render_data, width=0.8/24, colorup='green', colordown='red', alpha=0.8)

        # Put all dates to one list and fill ax2 sublot with volume
        Date_Render_range = [i[0] for i in self.render_data]
        self.ax2.clear()
        self.ax2.fill_between(Date_Render_range, self.Volume, 0)

        # draw our net_worth graph on ax3 (shared with ax1) subplot
        self.ax3.clear()
        self.ax3.plot(Date_Render_range, self.net_worth, color="blue")
        
        # beautify the x-labels (Our Date format)
        self.ax1.xaxis.set_major_formatter(self.date_format)
        self.fig.autofmt_xdate()

        # sort sell and buy orders, put arrows in appropiate order positions
        for trade in trades:
            trade_date = mpl_dates.date2num([pd.to_datetime(trade['Date'])])[0]
            if trade_date in Date_Render_range:
                if trade['type'] == 'buy':
                    high_low = trade['Low']-10
                    self.ax1.scatter(trade_date, high_low, c='green', label='green', s = 120, edgecolors='none', marker="^")
                else:
                    high_low = trade['High']+10
                    self.ax1.scatter(trade_date, high_low, c='red', label='red', s = 120, edgecolors='none', marker="v")

        # we need to set layers every step, because we are clearing subplots every step
        self.ax2.set_xlabel('Date')
        self.ax1.set_ylabel('Price')
        self.ax3.set_ylabel('Balance')

        # I use tight_layout to replace plt.subplots_adjust
        self.fig.tight_layout()

        """Display image with matplotlib - interrupting other tasks"""
        # Show the graph without blocking the rest of the program
        #plt.show(block=False)
        # Necessary to view frames before they are unrendered
        #plt.pause(0.001)

        """Display image with OpenCV - no interruption"""
        # redraw the canvas
        self.fig.canvas.draw()
        # convert canvas to image
        img = np.fromstring(self.fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')
        img  = img.reshape(self.fig.canvas.get_width_height()[::-1] + (3,))

        # img is rgb, convert to opencv's default bgr
        image = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)

        # display image with OpenCV or any operation you like
        cv2.imshow("Bitcoin trading bot",image)

        if cv2.waitKey(25) & 0xFF == ord("q"):
            cv2.destroyAllWindows()
            return
```

또한 내가 기능을 칭찬했음을 알 수 있습니다 `plt.show(block=False)`. 대신 이미지를 표시하기 위해 cv2 함수를 작성했습니다. `matplotlib`모든 단계에서 시각화 를 사용하는 동안 모든 작업이 중단되었기 때문에 이 작업을 수행했습니다. 계속해서 코드를 작성하거나 최소화하거나 키보드로 입력할 수 없습니다. 해결 방법으로 가장 좋은 솔루션은 OpenCV 라이브러리를 사용하는 것입니다. 그리고 여기에 우리의 비트코인 거래 BOT가 작업을 수행하는 마지막 아름다운 시각화가 있습니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-visualization/gameplay_4.gif)Matplotlib로 생성된 작성자의 gif

# 결론:

그리고 그게 다야! 이제 이전 튜토리얼에서 생성한 무작위 비트코인 거래 환경의 아름다운 라이브 렌더링 시각화가 생겼습니다! 에이전트에게 돈을 버는 방법을 가르치는 데 아직 많은 시간을 투자하지 않았습니다... 다음 튜토리얼에서 그렇게 할 것입니다!

우리는 여기서 엄청난 일을 해냈고 이제 훈련된 에이전트가 몇 가지 특정 작업을 수행하도록 하면 이러한 작업이 무작위보다 나은지 아닌지를 평가할 수 있습니다.

읽어 주셔서 감사합니다! 에이전트의 두뇌에 일부 뉴런을 추가하는 다음 부분에서 뵙겠습니다! 항상 그렇듯이 이 튜토리얼에서 제공하는 모든 코드는 내 [GitHub](https://github.com/pythonlessons/RL-Bitcoin-trading-bot) 페이지에서 찾을 수 있으며 무료로 사용할 수 있습니다!



---

# 시장을 이길 수 있는 비트코인 거래 봇 만들기 #3

이 튜토리얼에서는 비트코인 거래 봇을 계속 개발할 것이지만 이번에는 무작위로 거래하는 대신 강화 학습의 힘을 사용할 것입니다.

이 튜토리얼에서는 비트코인 거래 봇을 계속 개발할 것이지만 이번에는 무작위로 거래하는 대신 강화 학습의 힘을 사용할 것입니다.

[이전](https://pylessons.com/RL-BTC-BOT-visualization/) 튜토리얼과 이 튜토리얼 의 목적은  최첨단 심층 강화 학습 기술을 실험하여 수익성 있는 비트코인 거래 봇을 만들 수 있는지 확인하는 것입니다. 인터넷의 많은 기사에서는 신경망이 시장을 이길 수 없다고 말합니다. 그러나 이 분야의 최근 발전은 RL 에이전트가 종종 동일한 문제 영역 내에서 지도 학습 에이전트보다 훨씬 더 많이 학습할 수 있음을 보여주었습니다. 이러한 이유로 저는 가능하다면, 그리고 만약 그렇다면 우리가 이 거래 봇을 얼마나 수익성 있게 만들 수 있는지 실험하기 위해 이 튜토리얼을 작성하고 있습니다.

[OpenAI](https://openai.com/blog/) 엔지니어들이 했던 것만 큼 인상적인 것을 만들지는 않겠지 만, 비트코인을 일상적으로 수익성 있게 거래하고 이익을 내는 것은 여전히 쉬운 일이 아닙니다!

## 이 시점까지 계획

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-NN/trading_plan.png)작성자별 이미지

계속 진행하기 전에 목표를 달성하기 위해 해야 할 일에 대해 알아보겠습니다.

- [에이전트가 배울 수 있는 맞춤형 거래 환경을 만드십시오.](https://pylessons.com/RL-BTC-BOT-backbone/)
- [해당 환경에 대한 간단하고 우아한 시각화를 렌더링합니다.](https://pylessons.com/RL-BTC-BOT-visualization/)
- 수익성 있는 거래 전략을 배우기 위해 강화 학습 에이전트를 교육하십시오(현재).

이전 튜토리얼에 아직 익숙하지 않다면 [ 처음부터 맞춤형 암호화 거래 환경을 만드십시오 — Bitcoin 거래 봇 예제](https://pylessons.com/RL-BTC-BOT-backbone/) 및 [Matplotlib를 사용하여 우아한 Bitcoin RL 거래 에이전트 차트 시각화](https://pylessons.com/RL-BTC-BOT-visualization/) . 얼마 전 나는 이 두 가지 주제에 대한 튜토리얼을 작성했습니다. 이 (세 번째) 튜토리얼을 계속하기 전에 여기에서 잠시 멈추고 둘 중 하나를 읽으십시오.

# 구현

이 자습서의 이전 자습서에서 사용한 것과 동일한 시장 기록 데이터를 사용합니다. 내가 얻은 곳을 놓쳤다면 여기 링크 [https://bitcoincharts.com/](https://bitcoincharts.com/charts/bitstampUSD#rg150zigHourlyztgSzm1g10zm2g25zv) 입니다. .csv 파일은  이 전체 자습서 코드와 함께 내 [GitHub 리포지토리에서도 사용할 수 있습니다. ](https://github.com/pythonlessons/RL-Bitcoin-trading-bot/tree/main/RL-Bitcoin-trading-bot_3)테스트하기 전에 테스트하고 싶다면 이 튜토리얼을 읽는 것이 좋습니다. 자, 시작하겠습니다.

이 튜토리얼에서 코드를 작성하면서 파이썬을 처음 시작하는 사람들에게는 필수 라이브러리가 복잡할 수 있다고 생각하여 사람들이 어떤 패키지를 설치해야 하는지 알 수 있도록 [GitHub](https://github.com/pythonlessons/RL-Bitcoin-trading-bot/tree/main/RL-Bitcoin-trading-bot_3)`requirements.txt` 저장소에 파일을 추가하기로 결정했습니다. 따라서 내 코드를 복제하는 경우 테스트하기 전에 이 자습서에 필요한 모든 패키지를 설치하는 명령을 실행해야 합니다.`pip install -r ./requirements.txt`



```python
numpy
tensorflow==2.3.1
tensorflow-gpu==2.3.1
opencv-python
matplotlib
tensorboardx
pandas
mplfinance
```

이 (세 번째) 튜토리얼 부분에는 사용자 정의 프로그래밍 창의성이 거의 필요하지 않다고 말하고 싶습니다. 나는 이미 과거에 필요한 모든 것을 프로그래밍했습니다. 약간의 수정으로 두 개의 다른 코드를 병합해야 합니다. 저를 팔로우하고 계셨다면 저는 이미 [Gym LunarLander-v2](https://pylessons.com/LunarLander-v2-PPO/) 환경을 위한 PPO(Proximal Policy Optimization) 강화 학습 에이전트를 작성하고 테스트했습니다. [따라서 해당 코드를 선택하고 이전](https://pylessons.com/RL-BTC-BOT-visualization/) 자습서 코드 와 병합해야 합니다.



PPO에 익숙하지 않은 경우 이전 LunarLander-v2 자습서를 읽는 것이 좋습니다. 이것은 우리가 여기서 무엇을 하고 있는지에 대한 아이디어를 형성하는 데 도움이 될 것입니다. 이전 튜토리얼과 달리 `model.py`. 따라서 다음 파일에 `Actor_Model`및 클래스를 복사 하고 파일 시작 부분에 코드를 빌드하는 데 필요한 모든 가져오기를 추가합니다.`Critic_Model`

```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Flatten
from tensorflow.keras import backend as K
#tf.config.experimental_run_functions_eagerly(True) # used for debuging and development
tf.compat.v1.disable_eager_execution() # usually using this for fastest performance

gpus = tf.config.experimental.list_physical_devices('GPU')
if len(gpus) > 0:
    print(f'GPUs {gpus}')
    try: tf.config.experimental.set_memory_growth(gpus[0], True)
    except RuntimeError: pass

class Actor_Model:
    def __init__(self, input_shape, action_space, lr, optimizer):
        X_input = Input(input_shape)
        self.action_space = action_space

        X = Flatten(input_shape=input_shape)(X_input)
        X = Dense(512, activation="relu")(X)
        X = Dense(256, activation="relu")(X)
        X = Dense(64, activation="relu")(X)
        output = Dense(self.action_space, activation="softmax")(X)

        self.Actor = Model(inputs = X_input, outputs = output)
        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(lr=lr))

    def ppo_loss(self, y_true, y_pred):
        # Defined in https://arxiv.org/abs/1707.06347
        advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]
        LOSS_CLIPPING = 0.2
        ENTROPY_LOSS = 0.001
        
        prob = actions * y_pred
        old_prob = actions * prediction_picks

        prob = K.clip(prob, 1e-10, 1.0)
        old_prob = K.clip(old_prob, 1e-10, 1.0)

        ratio = K.exp(K.log(prob) - K.log(old_prob))
        
        p1 = ratio * advantages
        p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages

        actor_loss = -K.mean(K.minimum(p1, p2))

        entropy = -(y_pred * K.log(y_pred + 1e-10))
        entropy = ENTROPY_LOSS * K.mean(entropy)
        
        total_loss = actor_loss - entropy

        return total_loss

    def predict(self, state):
        return self.Actor.predict(state)

class Critic_Model:
    def __init__(self, input_shape, action_space, lr, optimizer):
        X_input = Input(input_shape)

        V = Flatten(input_shape=input_shape)(X_input)
        V = Dense(512, activation="relu")(V)
        V = Dense(256, activation="relu")(V)
        V = Dense(64, activation="relu")(V)
        value = Dense(1, activation=None)(V)

        self.Critic = Model(inputs=X_input, outputs = value)
        self.Critic.compile(loss=self.critic_PPO2_loss, optimizer=optimizer(lr=lr))

    def critic_PPO2_loss(self, y_true, y_pred):
        value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss
        return value_loss

    def predict(self, state):
        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])
```



내가 말했듯이 PPO 모델이 어떻게 작동하는지 이미 다루었기 때문에 설명하지 않겠습니다. `X = Dense(512, activation=”relu”)(X_input)`그러나 입력 으로 사용하는 대신 코드에서 한 가지 차이점을 알 수 있습니다 `X = Flatten(input_shape=input_shape)(X_input)`. 모델 입력 모양이 3D 모양(1, 50, 10)이고 첫 번째 Dense 레이어가 이것을 이해하지 못하기 때문에 그렇게 하는 것입니다. 그래서 Flatten 레이어를 사용하여 모양의 연결된 배열을 제공합니다(1, 500 ) — 이것은 우리 모델이 배우려고 하는 값입니다.

좋아, 내 메인 스크립트로 돌아가면 많은 업그레이드가 있습니다. 우선 기존 가져오기에 세 가지를 더 추가합니다.

```python
from tensorboardX import SummaryWriter
from tensorflow.keras.optimizers import Adam, RMSprop
from model import Actor_Model, Critic_Model
```

TensorboardX는 Tensorboard 로그에 사용됩니다. 사용 여부는 사용자에게 달려 있지만 때로는 유용합니다. 다음으로 실험을 위해 Adam 및 RMSprop 옵티마이저를 가져와서 기본 스크립트에서 다른 옵티마이저를 변경하고 실험합니다. 마지막으로 새로 만든 클래스 `Actor`와 `Critic`클래스를 가져옵니다.

기본 스크립트에서 `CustomEnv`클래스 초기화 부분에 다음 코드를 추가합니다.

```python
def __init__(self, df, initial_balance=1000, lookback_window_size=50, Render_range = 100):
    ...
    
    # Neural Networks part bellow
    self.lr = 0.0001
    self.epochs = 1
    self.normalize_value = 100000
    self.optimizer = Adam

    # Create Actor-Critic network model
    self.Actor = Actor_Model(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer)
    self.Critic = Critic_Model(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer)

    # create tensorboard writer
    def create_writer(self):
        self.replay_count = 0
        self.writer = SummaryWriter(comment="Crypto_trader")
```



여기에서 학습률, 훈련 에포크 및 신경망에 대해 선택한 옵티마이저를 정의합니다. 또한 여기에서는 일반적으로 권장되고 때로는 매우 중요한 정규화/스케일링 값을 정의합니다. 특히 신경망의 경우 정규화는 활성화 함수에 비정규화된 입력을 입력할 때 도메인의 매우 평평한 영역에 갇혀 전혀 학습하지 못할 수 있기 때문에 매우 중요할 수 있습니다. 또는 더 나쁘게는 숫자 문제로 끝납니다. 나중에 이 정규화 항목에 대해 더 깊이 알아볼 필요가 있지만 이제 데이터 세트에 더 큰 숫자가 없다는 것을 알기 때문에 100000 값을 사용하겠습니다. 가장 좋은 경우에는 최소값과 최대값 사이의 정규화 값을 사용해야 하지만, 미래 시장 최고가가 과거보다 더 커지면 어떻게 될까요? 실제로 정규화를 통해 앞으로 답변해야 할 답변보다 질문이 더 많습니다. 이제 이 하드코딩된 정규화를 사용하겠습니다.

비록 우리가 이 장소에서 우리 `Actor`와 `Critic`클래스를 생성하지만, 그것은 우리를 위해 모든 복잡한 작업을 수행할 것입니다! 재생 카운터와 작성기는 Tensorboard 로깅에 사용됩니다. 그렇게 중요한 것은 없습니다.

`replay`다음 으로 `act`, `save`및 `load`함수 를 복사 했습니다. 모두 바뀌지 않았습니다. 를 제외하고 `replay`저는 Tensorboard 작성자가 마지막에 배우와 비평가 손실을 기록하는 데 사용되는 세 줄을 추가했습니다. 또한 함수 에서 단계 함수에서 사용 `reset`하는 매개 변수를 추가 했다는 것을 언급하는 것을 잊었습니다. `self.episode_orders`에이전트가 주문을 판매하거나 구매할 때마다 저는 1씩 증가 `self.episode_orders`합니다. 이 매개변수를 사용하여 에이전트가 한 에피소드를 통해 수행하는 주문 수를 추적합니다.

## 에이전트 교육

시장에서 수익성 있는 거래를 하기 위해 이 에이전트를 어떻게 훈련시키나요? 이 부분이 가장 기대되는 부분인 것 같아요. 일반적으로 강화 학습을 처음 접하는 모든 사람들은 구체적인 환경에서 문제를 해결하기 위해 에이전트를 준비하는 방법을 모르므로 간단한 문제부터 시작하여 작은 단계를 수행하면서 더 나은 점수를 얻어 더 어려운 환경을 시도하는 것이 좋습니다. 이것이 내가 첫 번째 튜토리얼에서 무작위 거래를 구현한 이유입니다. 이제 우리는 그것을 사용하여 에이전트에게 합리적인 조치를 제공할 수 있습니다. 에이전트를 훈련시키는 코드는 다음과 같습니다.

```python
def train_agent(env, visualize=False, train_episodes = 50, training_batch_size=500):
    env.create_writer() # create TensorBoard writer
    total_average = deque(maxlen=100) # save recent 100 episodes net worth
    best_average = 0 # used to track best average net worth
    for episode in range(train_episodes):
        state = env.reset(env_steps_size = training_batch_size)

        states, actions, rewards, predictions, dones, next_states = [], [], [], [], [], []
        for t in range(training_batch_size):
            env.render(visualize)
            action, prediction = env.act(state)
            next_state, reward, done = env.step(action)
            states.append(np.expand_dims(state, axis=0))
            next_states.append(np.expand_dims(next_state, axis=0))
            action_onehot = np.zeros(3)
            action_onehot[action] = 1
            actions.append(action_onehot)
            rewards.append(reward)
            dones.append(done)
            predictions.append(prediction)
            state = next_state
            
        env.replay(states, actions, rewards, predictions, dones, next_states)
        total_average.append(env.net_worth)
        average = np.average(total_average)
        
        env.writer.add_scalar('Data/average net_worth', average, episode)
        env.writer.add_scalar('Data/episode_orders', env.episode_orders, episode)
        
        print("net worth {} {:.2f} {:.2f} {}".format(episode, env.net_worth, average, env.episode_orders))
        if episode > len(total_average):
            if best_average < average:
                best_average = average
                print("Saving model")
                env.save()
```



또한 훈련 부분을 차근차근 설명하지 않겠습니다. 여기의 모든 단계를 이해하려면 내 [강화 학습 자습서](https://pylessons.com/LunarLander-v2-PPO/) 를 확인하세요 . 그러나 나는 짧은 통지를 할 것입니다. 다음 라인을 사용하여 순자산 평균과 에이전트가 에피소드를 통해 수행하는 주문 수를 표시합니다.

```
env.writer.add_scalar('Data/average net_worth', average, episode)
env.writer.add_scalar('Data/episode_orders', env.episode_orders, episode)
```

위의 라인은 에이전트가 학습하는 방법을 확인하는 데 도움이 됩니다. 또한 매 단계마다 모델을 저장하는 대신 100개의 에피소드를 통해 모델이 달성할 수 있는 최고 평균 점수를 추적하고 최고만 유지합니다. 또한 이것이 적절한 평가 방법인지 확실하지 않지만 우리는 보게 될 것입니다.

## 에이전트 테스트

에이전트 테스트는 교육만큼 중요하거나 더 관련성이 있으므로 에이전트를 테스트하는 방법을 알아야 합니다. 테스트 에이전트 기능은 무작위 게임 에이전트와 매우 유사합니다.

```python
def test_agent(env, visualize=True, test_episodes=10):
    env.load() # load the model
    average_net_worth = 0
    for episode in range(test_episodes):
        state = env.reset()
        while True:
            env.render(visualize)
            action, prediction = env.act(state)
            state, reward, done = env.step(action)
            if env.current_step == env.end_step:
                average_net_worth += env.net_worth
                print("net_worth:", episode, env.net_worth, env.episode_orders)
                break
            
    print("average {} episodes agent net_worth: {}".format(test_episodes, average_net_worth/test_episodes))
```

두 가지 차이점이 있습니다.

- 함수 시작 시 훈련된 모델 가중치를 로드합니다.
- 둘째, 무작위 행동 `(action = np.random.randint(3, size=1)[0])`을 하는 대신 훈련된 모델을 사용하여 행동을 예측합니다 `(action, prediction = env.act(state))`.

# 재미가 시작됩니다 - 훈련 부분:

다른 사람들이 시장 예측 스크립트를 작성할 때 저지르는 가장 큰 실수 중 하나는 데이터를 훈련 및 테스트 세트로 분할하지 않는다는 것입니다. 모델이 본 데이터에서 훌륭하게 수행될 것이 분명하다고 생각합니다. 데이터 세트를 훈련과 테스트로 분리하는 목적은 이전에 본 적이 없는 새로운 데이터에 대한 최종 모델의 정확도를 테스트하는 것입니다. 시계열 데이터를 사용하기 때문에 교차 검증을 위한 옵션이 많지 않습니다.

예를 들어, 교차 검증의 한 가지 일반적인 형태는 k-겹 검증이라고 합니다. 데이터를 k 개의 동일한 그룹으로 분할하고 한 그룹을 테스트 그룹으로 하나씩 선택하고 나머지 정보는 교육 그룹으로 사용합니다. 그러나 시계열 데이터는 시간 종속성이 높기 때문에 이후 데이터는 이전 데이터에 크게 의존합니다. 따라서 에이전트가 거래하기 전에 미래 데이터에서 학습하기 때문에 k-fold는 작동하지 않습니다. 그것은 부당한 이점입니다.



따라서 프레임의 시작 부분부터 임의의 인덱스까지 훈련 세트로 사용할 전체 데이터 프레임의 조각을 가져오고 나머지 데이터를 테스트 세트로 사용합니다.

```python
df = pd.read_csv(‘./pricedata.csv’)
df = df.sort_values(‘Date’)
lookback_window_size = 50
train_df = df[:-720-lookback_window_size]
test_df = df[-720-lookback_window_size:] # 30 days
```

우리 환경은 단일 데이터 프레임만 처리하도록 설정되어 있으므로 훈련 데이터와 테스트 데이터를 위한 두 가지 환경을 생성합니다.

```python
train_env = CustomEnv(train_df,lookback_window_size=lookback_window_size)
test_env = CustomEnv(test_df,lookback_window_size=lookback_window_size)
```

이제 모델을 훈련시키는 것은 우리 환경으로 에이전트를 생성하고 위에서 생성한 훈련 함수를 호출하는 것만큼 간단합니다.

```python
train_agent(train_env, visualize=False, train_episodes=20000, training_batch_size=500)
```

언제까지 훈련해야 하는지 잘 모르겠지만 20,000보를 훈련하기로 했습니다. 터미널에 다음 명령을 작성 하고 브라우저에서 http://localhost:6006/`tensorboard --logdir runs` 을 열어 훈련 프로세스가 Tensorboard에서 어떻게 보이는지 봅시다 .

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-NN/tensorboard_results.png)작성자의 Tensorboard 이미지

위 이미지는 20,000개의 훈련 에피소드에 대해 에이전트를 훈련하는 동안의 Tensorboard 결과를 보여줍니다. 보시다시피 배우 손실은 올라가고 평론가 손실은 줄어들고 시간이 지남에 따라 안정화되는 것이 좋습니다. 그러나 나에게 가장 흥미로운 차트 중 하나는 에피소드 주문과 평균 순자산입니다. 예, 평균 순자산이 증가했지만 초기 잔액에서 몇 퍼센트만 증가했음을 알 수 있습니다. 그러나 여전히 그것은 이익입니다! 위의 그래프는 우리 에이전트가 학습 중임을 알려주며, 보시다시피 에이전트는 많은 것보다 적은 주문을 하는 것이 낫다고 생각했습니다. 한 교육 순간에 에이전트 이벤트가 최고의 거래 전략이 유지된다는 것을 가져왔지만 우리는 그렇게 하고 싶지 않습니다. 위의 차트에서 내가 대답하려고 했던 것은 우리 에이전트가 뭔가를 배울 수 있습니까? 대답은 - 예! 나는 우리가 YES라고 말할 수 있다고 100% 확신합니다. 에이전트가 무언가를 배우고 있습니다.

# 보이지 않는 데이터로 테스트

먼저 에이전트가 이전에 본 적이 없는 데이터에 대해 수행하는 방식을 살펴보겠습니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-NN/gameplay.gif)작성자의 GIF

이것은 짧은 GIF에 불과하지만 더 보고 싶다면 모든 것을 보여주고 설명하는 내 YouTube 비디오를 시청하거나 내 [GitHub](https://github.com/pythonlessons/RL-Bitcoin-trading-bot/tree/main/RL-Bitcoin-trading-bot_3) 저장소를 복제하고 이 에이전트를 직접 테스트하는 것이 가장 좋습니다.

좋습니다. 에이전트를 평가하고 다음 두 명령을 사용하여 1000개의 에피소드 단계에서 임의의 에이전트를 이길 수 있는지 확인하겠습니다.

```python
test_agent(test_env, visualize=False, test_episodes=1000)
Random_games(test_env, visualize=False, train_episodes = 1000)
```

훈련된 에이전트의 결과는 다음과 같습니다.

```
average 1000 episodes agent net_worth: 1043.4012850463675
```

무작위 에이전트 결과는 다음과 같습니다.

```
average 1000 episodes random net_worth: 1024.3194149765457
```

랜덤 에이전트도 좋은 수익을 얻은 유일한 이유는 시장 추세가 상승했기 때문입니다. 우리 에이전트의 이 평균 수익이 한 달 만에 발생한 것을 고려하면 4%는 꽤 좋은 결과로 들립니다. 그러나 또한 위의 내 거래 에이전트 gif를 보면 동작이 매우 이상합니다. 에이전트는 제로 오더를 유지하는 것을 좋아하지 않습니다. 그러나 에이전트가 열려 있는 모든 포지션을 매도한 직후에 또 다른 매수를 시작한다는 사실을 알게 되었습니다. 이것은 결코 좋은 이익으로 이어지지 않으므로 이 문제를 분석하고 해결해야 합니다.

# 결론:

이것은 꽤 긴 튜토리얼이었고 여기서 멈추겠습니다. 우리는 시장을 이기기 위해 심층 강화 학습을 사용하여 처음부터 수익성 있는 비트코인 거래 에이전트를 만드는 목표를 달성했습니다! 우리는 이미 다음을 달성했습니다.

- [Bitcoin 거래 환경을 처음부터 만들었습니다.](https://pylessons.com/RL-BTC-BOT-backbone/)
- [해당 환경의 렌더링된 시각화를 구축했습니다.](https://pylessons.com/RL-BTC-BOT-visualization/)
- 보이지 않는 데이터로 에이전트를 교육하고 테스트했습니다.
- 우리 에이전트는 무작위 에이전트보다 약간 더 나은 성과를 거두었고 약간의 이익을 얻었습니다!

우리의 거래 에이전트는 우리가 기대한 만큼 수익성이 좋지는 않지만 어딘가에 가고 있습니다. 다음 튜토리얼에서 무엇을 시도할지 확신할 수 없지만 몇 가지 다른 보상 전략을 테스트하고 일부 모델 최적화를 수행할 것이라고 확신합니다. 하지만 우리는 보이지 않는 데이터에서 더 나은 수익을 얻어야 한다는 것을 알고 있으므로 작업하겠습니다!

읽어 주셔서 감사합니다! 에이전트를 개선하기 위해 더 많은 기술을 사용하려고 시도하는 다음 부분에서 뵙겠습니다! 항상 그렇듯이 이 튜토리얼에서 제공하는 모든 코드는 내 [GitHub](https://github.com/pythonlessons/RL-Bitcoin-trading-bot) 페이지에서 찾을 수 있으며 무료로 사용할 수 있습니다!

***이 모든 튜토리얼은 교육 목적을 위한 것이며 거래 조언으로 받아들여서는 안됩니다. 투자를 잃을 가능성이 있으므로 이 튜토리얼, 이전 또는 향후 튜토리얼에서 정의된 알고리즘이나 전략을 기반으로 거래하지 않는 것이 가장 좋습니다.***



---

# 비트코인 거래 봇 모델 최적화 및 수익성 향상을 위한 보상 전략 #4

## 더 나은 보상 전략으로 더 많은 돈을 벌고 다양한 모델 구조를 테스트할 수 있도록 deep RL Bitcoin 거래 에이전트 코드를 개선합시다.

더 나은 보상 전략으로 더 많은 돈을 벌고 다양한 모델 구조를 테스트할 수 있도록 deep RL Bitcoin 거래 에이전트 코드를 개선합시다.

[이전 튜토리얼](https://pylessons.com/RL-BTC-BOT-NN/) 에서 우리 는 시장을 이길 수 있는 비트코인 거래 에이전트를 만들기 위해 심층 강화 학습을 사용했습니다. 우리 에이전트는 무작위 행동에 비해 수익성이 있었지만 결과는 그다지 인상적이지 않았으므로 이번에는 이를 한 단계 더 높이고 보상 시스템과 함께 몇 가지 개선 사항을 더 구현하려고 합니다. 우리의 수익성이 신경망 모델 구조에 어떻게 의존하는지 테스트할 것입니다. 그러나 우리는 우리의 코드로 모든 것을 테스트할 것이고, 물론 내 [GitHub](https://github.com/pythonlessons/RL-Bitcoin-trading-bot/tree/main/RL-Bitcoin-trading-bot_4) 리포지토리에서 모든 것을 다운로드할 수 있을 것입니다.

# 보상 최적화

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-reward/risk_reward.png)보상 전략을 작성하면서 자동 거래 전략을 구현하면서 다른 사람들이 강화 학습에서 어떤 보상 전략을 사용하는지 찾기가 꽤 어려웠다고 언급하고 싶습니다. 다른 사람들이 하는 일을 보는 것은 꽤 어렵습니다. 내가 찾을 수만 있다면 이러한 전략은 문서화되지 않았고 이해하기가 상당히 복잡했습니다. 흥미롭고 성공적인 전략 솔루션이 많이 있다고 생각하지만 이 자습서에서는 직관에 의존하여 전략을 시도하기로 결정했습니다.

누군가는 이전 튜토리얼의 보상 함수(즉, 각 단계에서 순자산의 변화를 계산하는 것)가 우리가 할 수 있는 최선이라고 생각할 수 있습니다. 그러나 그것은 진실과 거리가 멀다. 우리의 간단한 보상 기능이 지난번에는 약간의 이익을 냈을지 모르지만 종종 자본 손실로 이어질 것입니다. 이를 개선하기 위해 단순히 실현되지 않은 이익 외에도 다른 보상 메트릭을 고려해야 합니다.



내 머리에 오는 주요 개선 사항은 가격이 상승하면서 BTC를 보유하여 이익을 보상하고 가격이 하락하는 동안 BTC를 보유하지 않은 이익을 보상해야 한다는 것입니다. 아이디어 중 하나는 BTC/USD 포지션을 보유하는 동안 순자산이 점진적으로 증가하는 경우 에이전트에게 보상을 제공하고 열린 포지션이 없는 동안 BTC/USD 가치가 누적 감소한 경우 에이전트에게 다시 보상할 수 있다는 것입니다.

그러나 우리는 보유하고 있는 비트코인을 판매하거나 가격이 하락한 후 비트코인을 구매할 때 보상을 계산하여 이를 구현할 것입니다. 주문 사이에 에이전트는 아무 작업도 수행하지 않지만 이러한 보상은 할인 기능으로 계산되기 때문에 사용하지 않습니다. 그래서 제가 이 보상 전략을 사용했을 때 제 에이전트는 보통 수익성 있는 주문을 하는 법을 배우는 대신 보류하는 법을 배운다는 것을 알아차리고 아무것도 하지 않은 것에 대해 벌을 주기로 결정했습니다. 나는 매 단계마다 순자산의 0.01%를 뺍니다. 이런 식으로 우리 에이전트는 비트코인을 계속 보유하거나 포지션을 영원히 유지하는 것이 최선의 아이디어가 아니라는 것을 배웠습니다. 또한 에이전트는 때로는 손실을 줄이고 주문할 다른 기회를 기다리는 것이 더 낫다는 것을 이해했습니다.

사용자 정의 보상 기능의 코드는 다음과 같습니다.

```python
# Calculate reward
def get_reward(self):
    self.punish_value += self.net_worth * 0.00001
    if self.episode_orders > 1 and self.episode_orders > self.prev_episode_orders:
        self.prev_episode_orders = self.episode_orders
        if self.trades[-1]['type'] == "buy" and self.trades[-2]['type'] == "sell":
            reward = self.trades[-2]['total']*self.trades[-2]['current_price'] - self.trades[-2]['total']*self.trades[-1]['current_price']
            reward -= self.punish_value
            self.punish_value = 0
            self.trades[-1]["Reward"] = reward
            return reward
        elif self.trades[-1]['type'] == "sell" and self.trades[-2]['type'] == "buy":
            reward = self.trades[-1]['total']*self.trades[-1]['current_price'] - self.trades[-2]['total']*self.trades[-2]['current_price']
            reward -= self.punish_value
            self.trades[-1]["Reward"] = reward
            self.punish_value = 0
            return reward
    else:
        return 0 - self.punish_value
```

이전 자습서나 전체 코드에 익숙해지지 않고 이 자습서를 읽고 있다고 가정합니다. 이 경우 이 코드를 이해하지 못할 수 있으므로 [GitHub](https://github.com/pythonlessons/RL-Bitcoin-trading-bot) 또는 [이전 자습서](https://pylessons.com/RL-BTC-BOT-NN/) 에서 내 코드를 확인  하여 익숙해지십시오.



위의 코드에서 알 수 있듯이 우리가 가장 먼저 하는 일은 매 단계마다 누적되는 벌칙 값을 계산합니다. 주문이 이루어지면 0으로 설정합니다. if 문을 살펴보면 그 중 두 가지를 볼 수 있습니다. 매도 직후 매수와 그 반대입니다. 왜 가끔 `self.trades[-2]`과 때때로 를 사용하는지 물어볼 수 있습니다 `self.trades[-1]`. 이것은 열려 있지 않은 주문의 보상을 계산하기를 원하기 때문에 수행됩니다. 비트코인을 팔고 나중에 사면(마진 거래 제외) 실제 이익을 얻을 수 없기 때문에 위와 같은 방법으로 고가에서 저가로 사면서 잃지 않은 것을 계산할 수 있습니다.

이 전략을 개발하는 동안 올바르게 구현했는지 이해하기가 꽤 까다로워서 다음 코드로 `render`기능 을 개선하기로 결정했습니다.`utils.py`

```python
# sort sell and buy orders, put arrows in appropiate order positions
for trade in trades:
    trade_date = mpl_dates.date2num([pd.to_datetime(trade['Date'])])[0]
    if trade_date in Date_Render_range:
        if trade['type'] == 'buy':
            high_low = trade['Low']-10
            self.ax1.scatter(trade_date, high_low, c='green', label='green', s = 120, edgecolors='none', marker="^")
        else:
            high_low = trade['High']+10
            self.ax1.scatter(trade_date, high_low, c='red', label='red', s = 120, edgecolors='none', marker="v")
```

더 동적인 코드:

```python
minimum = np.min(np.array(self.render_data)[:,1:])
maximum = np.max(np.array(self.render_data)[:,1:])
RANGE = maximum - minimum

# sort sell and buy orders, put arrows in appropiate order positions
for trade in trades:
    trade_date = mpl_dates.date2num([pd.to_datetime(trade['Date'])])[0]
    if trade_date in Date_Render_range:
        if trade['type'] == 'buy':
            high_low = trade['Low'] - RANGE*0.02
            ycoords = trade['Low'] - RANGE*0.08
            self.ax1.scatter(trade_date, high_low, c='green', label='green', s = 120, edgecolors='none', marker="^")
        else:
            high_low = trade['High'] + RANGE*0.02
            ycoords = trade['High'] + RANGE*0.06
            self.ax1.scatter(trade_date, high_low, c='red', label='red', s = 120, edgecolors='none', marker="v")

        if self.Show_reward:
            try:
                self.ax1.annotate('{0:.2f}'.format(trade['Reward']), (trade_date-0.02, high_low), xytext=(trade_date-0.02, ycoords),
                                           bbox=dict(boxstyle='round', fc='w', ec='k', lw=1), fontsize="small")
            except:
                pass
```

보시다시피 이제 하드코딩된 오프셋 withing 을 사용하는 대신 `high_low`해당 범위를 계산합니다. 이렇게 하면 오프셋을 수정하지 않고 다른 거래 쌍(아직 테스트되지 않음)에 대해 동일한 렌더링 그래프를 사용할 수 있습니다. 그러나 가장 중요한 것은 구매 주문 화살표 아래와 판매 주문 화살표 위에 보상 번호를 추가한다는 것입니다. 이것은 내가 보상 기능을 올바르게 구현했는지 이해하는 데 도움이 되었습니다. 한 번 살펴보세요.



![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-reward/new_reward_strategy.png)이 차트를 보면 우리 에이전트가 주문을 잘한 곳과 형편없는 곳이 분명합니다. 이것은 에이전트를 교육하는 데 사용할 정보입니다.

# 모델 수정

우리는 아마도 우리의 결정이 우리가 삶에서 내리는 좋은 결정이나 나쁜 결정, 새로운 것을 얼마나 빨리 배우는지, 목록이 계속될 수 있는지에 대한 지식에 주로 의존한다는 것을 알고 있을 것입니다. 이 모든 것은 우리의 뇌 기능에 달려 있습니다. 모델도 마찬가지입니다. 모든 것은 우리의 두뇌와 그것이 얼마나 잘 훈련되었는지에 달려 있습니다. 가장 큰 문제는 시장을 이기기 위해 우리 모델에 어떤 아키텍처를 사용해야 하는지 모른다는 것입니다. 남은 유일한 방법은 다른 아키텍처를 시도하는 것입니다.

우리는 이미 Actor 및 Critic 신경망에 Dense 기본 레이어를 사용하고 있습니다.



```python
# Critic model
X = Flatten()(X_input)
V = Dense(512, activation="relu")(X)
V = Dense(256, activation="relu")(V)
V = Dense(64, activation="relu")(V)
value = Dense(1, activation=None)(V)

# Actor model
X = Flatten()(X_input)
A = Dense(512, activation="relu")(X)
A = Dense(256, activation="relu")(A)
A = Dense(64, activation="relu")(A)
```

이것은 우리가 시도한 주요 방법 중 하나입니다. 그러나 인터넷 어딘가에서 Actor와 Critic 사이에 공유 레이어를 갖는 것이 좋습니다.

```python
# Shared Dense layers:
X = Flatten()(X_input)
X = Dense(512, activation="relu")(X)

# Critic model
V = Dense(512, activation="relu")(X)
V = Dense(256, activation="relu")(V)
V = Dense(64, activation="relu")(V)
value = Dense(1, activation=None)(V)

# Actor model
A = Dense(512, activation="relu")(X)
A = Dense(256, activation="relu")(A)
A = Dense(64, activation="relu")(A)
```



이는 예를 들어 두 개의 첫 번째 신경망이 두 네트워크에서 모두 사용되어야 하고 다음(헤드) 레이어만 분리되어야 함을 의미합니다. 다음은 예시 이미지입니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-reward/shared_model_structure.png)공유 레이어가 있는 비평가-배우 아키텍처

또한 이미지 분류 및 탐지에 주로 사용되는 시계열 및 컨볼루션 신경망에 사용되는 순환 신경망을 사용하려고 합니다.

## 순환 네트워크

테스트해야 하는 명백한 변경 사항 중 하나는 이전 Dense 네트워크 대신 순환 또는 소위 LSTM(Long Short-Term Memory) 네트워크를 사용하도록 모델을 업데이트하는 것입니다. 순환 네트워크는 시간이 지남에 따라 내부 상태를 유지할 수 있으므로 슬라이딩 "돌아보기" 창을 사용하여 가격 조치의 이력을 캡처할 필요가 없습니다. 대신, 본질적으로 네트워크의 재귀적 특성에 의해 포착됩니다. 각 시간 단계에서 데이터 세트의 입력은 마지막 시간 단계의 출력과 함께 알고리즘으로 전달됩니다. 우리를 엉망으로 만들지 않도록 코드에서 아직 제거하지 않을 것이며 어떤 모델이 더 나은 성능을 발휘하는지 테스트할 수 있을 것입니다. 또한 LSTM 네트워크에 대해 잘 알지 못하기 때문에 스스로 올바르게 구현하는 방법을 모르지만 [여기 에서는](https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/)그것에 대한 꽤 유익한 소개입니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-reward/LSTM_1.png)[소스](https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/)

LSTM 모델의 이러한 구조를 통해 에이전트가 특정 데이터 관계를 "기억"하고 "잊을" 때마다 업데이트되는 내부 상태를 유지할 수 있습니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-reward/LSTM_2.png)[소스](https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/)

나는 LSTM에 대해 깊이 들어가지 않을 것입니다. 왜냐하면 제가 말했듯이 나는 그것에 익숙하지 않기 때문입니다. 그러나 저는 미래에 시계열 분석과 튜토리얼 시리즈를 할 계획이 있습니다. 따라서 LSTM 공유 레이어를 사용하면 모델이 다음과 같이 표시됩니다.

```python
# Shared LSTM layers:
X = LSTM(512, return_sequences=True)(X_input)
X = LSTM(256)(X)

# Critic model
V = Dense(512, activation="relu")(X)
V = Dense(256, activation="relu")(V)
V = Dense(64, activation="relu")(V)
value = Dense(1, activation=None)(V)

# Actor model
A = Dense(512, activation="relu")(X)
A = Dense(256, activation="relu")(A)
A = Dense(64, activation="relu")(A)
```

## 컨볼루션 네트워크

CNN은 지금까지 가장 일반적으로 채택된 딥 러닝 모델입니다. 한편, 문헌에서 CNN 구현의 대부분은 컴퓨터 비전 및 이미지 분석 문제를 해결하기 위해 선택되었습니다. CNN 모델의 성공적인 배포와 함께 새로운 CNN 복잡한 아키텍처가 발명되는 동안 모델 오류율은 수년에 걸쳐 계속 떨어지고 있습니다. 오늘날 거의 모든 컴퓨터 비전 연구자들은 어떤 식으로든 이미지 분류 문제에서 CNN을 구현합니다.

이 [논문](https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/) 에서 저자들은 알고리즘 거래를 위해 심층 컨볼루션 신경망의 힘을 활용할 수 있도록 1차원 금융 시계열을 2차원 이미지와 같은 데이터 표현으로 변환하는 새로운 접근 방식을 제안하는 연구가 소개되었습니다. 체계. 그러나 저자는 흥미로운 기사를 작성했으며 매우 인상적인 결과를 얻었습니다. 시계열 이미지에 대해 훈련된 제안된 CNN 모델은 LSTM 네트워크와 매우 유사하게 수행되었습니다. 때로는 더 좋고 때로는 더 나쁩니다. 그러나 중요한 개선 사항은 CNN이 모델을 훈련하는 데 많은 계산 능력과 시간이 필요하지 않기 때문에 에이전트를 훈련하고 다음 모델로 테스트한다는 것입니다.



```python
# Shared CNN layers:
X = Conv1D(filters=64, kernel_size=6, padding="same", activation="tanh")(X_input)
X = MaxPooling1D(pool_size=2)(X)
X = Conv1D(filters=32, kernel_size=3, padding="same", activation="tanh")(X)
X = MaxPooling1D(pool_size=2)(X)
X = Flatten()(X)

# Critic model
V = Dense(512, activation="relu")(X)
V = Dense(256, activation="relu")(V)
V = Dense(64, activation="relu")(V)
value = Dense(1, activation=None)(V)

# Actor model
A = Dense(512, activation="relu")(X)
A = Dense(256, activation="relu")(A)
A = Dense(64, activation="relu")(A)
```

# 기타 사소한 변경 사항

그래서 지금까지는 주로 보상과 모델 개선에 대해 이야기했습니다. 그러나 그 사이에 다른 모델을 훈련하고 테스트할 때 삶을 더 쉽게 만드는 다른 방법도 있습니다.

우선, 모델이 언제 과적합되고 이 기호에 대해 신호를 보내는지 모르기 때문에 모델을 저장하는 방법을 변경했습니다. 적어도 지금은 모든 최고의 모델을 유지하기로 결정했습니다. 따라서 이전 모델 위에 최고의 모델을 저장하는 대신 이러한 모델을 저장하고 평균 보상을 이름으로 사용할 새 폴더를 만들고 있습니다.

내 모델을 테스트할 때 모델이 매우 지저분해지고 모델의 최상의 결과가 기억나지 않는다는 것을 알게 되었습니다. 또한 모든 새 모델을 테스트/훈련하면서 설정한 모든 매개변수를 기억하기가 매우 어렵다는 것을 확인하여 정확한 모델 저장 위치에 Parameters.txt 파일을 생성하고 있습니다. 그래서 다음 매개변수를 텍스트 파일에 씁니다.

```python
params.write(f"training start: {current_date}\n")
params.write(f"initial_balance: {initial_balance}\n")
params.write(f"training episodes: {train_episodes}\n")
params.write(f"lookback_window_size: {self.lookback_window_size}\n")
params.write(f"lr: {self.lr}\n")
params.write(f"epochs: {self.epochs}\n")
params.write(f"batch size: {self.batch_size}\n")
params.write(f"normalize_value: {normalize_value}\n")
params.write(f"model: {self.comment}\n")
```

따라서 이제 내가 시작한 초기 잔액, 사용한 룩백 창, 학습에 사용한 학습률, 한 에피소드에 사용한 에포크 수, 사용한 정규화 값을 알 수 있습니다. 마지막으로 내가 사용하는 모델 유형을 작성할 수 있습니다. 이것은 내 테스트 결과를 훨씬 쉽게 만듭니다!



테스트 결과의 또 다른 필수 단계는 비교의 단순성을 위해 한 곳에 두는 것이 편리합니다. 그래서 내 코드에 다음 줄을 삽입했습니다.

```python
print("average {} episodes agent net_worth: {}, orders: {}".format(test_episodes, average_net_worth/test_episodes, average_orders/test_episodes))
print("No profit episodes: {}".format(no_profit_episodes))
# save test results to test_results.txt file
with open("test_results.txt", "a+") as results:
    results.write(f'{datetime.now().strftime("%Y-%m-%d %H:%M")}, {name}, test episodes:{test_episodes}')
    results.write(f', net worth:{average_net_worth/(episode+1)}, orders per episode:{average_orders/test_episodes}')
    results.write(f', no profit episodes:{no_profit_episodes}, comment: {comment}\n')
```

`net_worth`이 결과를 통해 에이전트가 모든 테스트 에피소드를 거래한 평균, 에피소드를 통해 평균적으로 몇 건의 주문을 수행했는지, 가장 중요한 측정항목 중 하나가 "무수익 에피소드" 인지 비교할 수 있습니다. 테스트를 통해 부정적인 측면에서 끝낸 에피소드. 또한 추가할 수 있는 메트릭이 많이 있지만 이제 필요한 최상의 항목을 비교하고 선택하는 것으로 충분합니다. 또한 이렇게 하면 대량의 모델을 한 번에 테스트하고 밤에 두고 아침에 결과를 확인할 수 있습니다.

또한 모델 저장 기능을 약간 수정했으며 이제 모델을 저장하는 동안 다음 기능을 사용하여 현재 저장된 모델 시간 단계에 대한 매개변수를 기록할 수 있습니다.

```python
def save(self, name="Crypto_trader", score="", args=[]):
    # save keras model weights
    self.Actor.Actor.save_weights(f"{self.log_name}/{score}_{name}_Actor.h5")
    self.Critic.Critic.save_weights(f"{self.log_name}/{score}_{name}_Critic.h5")

    # log saved model arguments to file
    if len(args) > 0:
        with open(f"{self.log_name}/log.txt", "a+") as log:
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            log.write(f"{current_time}, {args[0]}, {args[1]}, {args[2]}, {args[3]}, {args[4]}\n")
```

# 교육 및 테스트

## 모델 학습

이미 언급했듯이 동일한 매개변수를 사용하여 동일한 데이터 세트에서 모든 모델을 훈련할 것입니다. 모델 유형만 변경하겠습니다. 훈련 후에 훈련 기간을 비교하고 Tensorboard 훈련 그래프를 비교할 수 있으며 물론 훈련된 모델을 얻을 수 있습니다. 동일한 테스트 데이터 세트에서 이러한 모든 훈련된 모델을 테스트하고 보이지 않는 시장 데이터에서 성능을 확인할 것입니다! 세 가지 다른 모델 아키텍처(Dense, CNN 및 LSTM)가 있습니다. 이제 아키텍처 간의 첫 번째 비교를 수행하기 위해 시간과 1080TI GPU를 투자하겠습니다.

그래서 다음 코드 라인을 사용하여 가장 단순한 Dense 모델에서 모델 교육을 시작했습니다.

```python
if __name__ == "__main__":            
    df = pd.read_csv('./pricedata.csv')
    df = df.sort_values('Date')

    lookback_window_size = 50
    test_window = 720 # 30 days 
    train_df = df[:-test_window-lookback_window_size]
    test_df = df[-test_window-lookback_window_size:]
    
    # Create our custom Neural Networks model
    agent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00001, epochs=5, optimizer=Adam, batch_size = 32, comment="Dense")
    
    # Create and run custom training environment with following lines
    train_env = CustomEnv(train_df, lookback_window_size=lookback_window_size)
    train_agent(train_env, agent, visualize=False, train_episodes=50000, training_batch_size=500)
```

교육이 시작되면 Parameters.txt 파일이 있는 동일한 폴더에 현재 날짜와 시간으로 새 폴더가 생성되는 것을 볼 수 있습니다. 이 파일을 열면 현재 사용된 모든 설정을 볼 수 있습니다.

```python
training start: 2021–01–11 13:32
initial_balance: 1000
training episodes: 50000
lookback_window_size: 50
lr: 1e-05
epochs: 5
batch size: 32
normalize_value: 40000
model: Dense
training end: 2021–01–11 18:20
```

보시다시피 이 파일에는 모델을 훈련하는 데 사용한 모든 매개변수가 저장되어 있습니다. 내 모델을 학습시키는 데 시간이 얼마나 걸렸는지 알 수 있습니다. 이것은 나중에 가장 최적화된 모델을 찾거나 다른 모델을 수동으로 훈련시키려고 할 때 도움이 됩니다. 일반적으로 시간이 걸리기 때문입니다. 보시다시피, 어떤 매개변수를 사용했는지 잊어버리기 쉽습니다.

또한 파일이 생성된 것을 볼 수 있으며, 이때 `log.txt`저장된 모든 모델 통계가 여기에 저장됩니다. 이는 과적합되지 않은 최상의 모델을 찾는 데 도움이 될 수 있습니다.

에이전트가 저장하는 모든 모델은 동일한 디렉토리에 있습니다. 따라서 테스트할 때 모델의 올바른 디렉토리와 이름을 지정해야 합니다.

Bellow는 50k 훈련 단계에 대해 Dense(주황색) 및 CNN(파란색) 네트워크를 훈련하는 동안 Tensorboard에서 발췌한 것입니다. 안타깝지만 제 실수로 LSTM 트레이닝 그래프를 제거했습니다. 다시 훈련시키기에는 너무 오랜 시간이 걸렸습니다. 우리 모델이 조금 더 좋아지면 다음 튜토리얼에서 그렇게 할 것입니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-reward/Tensorboard.png)Dense 및 CNN 모델의 Tensorboard 그래프

지금 당장 Dense와 CNN 모델과 어떻게 훈련했는지 비교해 보겠습니다. 먼저 Average_net_worth 그래프를 살펴보겠습니다. 보시다시피 CNN 모델은 시간이 지나면서 훨씬 더 높은 보상을 받는 법을 배웠지만 이러한 결과를 맹목적으로 신뢰해서는 안 됩니다. CNN이 과적합될 수 있다고 생각합니다. 우선, 비평가 네트워크가 어떻게 훈련되었고 이 `critic_loss_per_replay`그래프가 CNN을 어떻게 보는지 매우 의심스러워 보입니다. 숫자는 Dense 모델을 훈련할 때보다 100배 더 큽니다.

둘째, `actor_loss_per_replay`그래프를 보면 Dense 모델을 훈련하는 동안 꽤 아름다운 곡선을 볼 수 있습니다. 우리는 이 곡선이 약 20k 에피소드 이후로 위쪽 방향으로 오고 있음을 알 수 있습니다. 그러나 동일한 그래프에서 CNN 훈련 액터 손실 곡선을 보는 동안 약간의 훈련 불안정성을 제공합니다. 그러나 좋아, 이것은 나의 예비 견해입니다. 우리는 두 모델을 모두 테스트할 것입니다. 하나는 곡선 상단에 있고 하나는 평균 순자산으로 가장 좋은 모델입니다.

또한 보상 전략에 처벌 값을 추가했기 때문에 평균 에피소드 주문 그래프를 보는 것이 매우 흥미롭습니다. 우리 모델이 이 처벌을 피하고 가능한 한 많은 주문을 수행하는 방법을 배우는 것은 매우 논리적입니다. 그러나 모든 단계에서 명령을 수행하는 대신 때때로 처벌을 받고 긍정적인 보상을 받을 수 있는 더 나은 명령 조건을 기다리는 것이 더 낫다는 것을 발견했습니다!

## 모델 테스트

이미 알고 있듯이 우리는 50,000개의 훈련 단계에 대해 세 가지 다른 모델(Dense, CNN, LSTM)을 훈련했습니다. 다음 코드를 사용하여 한 번에 모두 테스트할 수 있습니다.

```python
if __name__ == "__main__":            
    df = pd.read_csv('./pricedata.csv')
    df = df.sort_values('Date')

    lookback_window_size = 50
    test_window = 720 # 30 days 
    train_df = df[:-test_window-lookback_window_size]
    test_df = df[-test_window-lookback_window_size:]

    agent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00001, epochs=1, optimizer=Adam, batch_size = 32, model="Dense")
    test_env = CustomEnv(test_df, lookback_window_size=lookback_window_size, Show_reward=False)
    test_agent(test_env, agent, visualize=False, test_episodes=1000, folder="2021_01_11_13_32_Crypto_trader", name="1277.39_Crypto_trader", comment="")

    agent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00001, epochs=1, optimizer=Adam, batch_size = 32, model="CNN")
    test_env = CustomEnv(test_df, lookback_window_size=lookback_window_size, Show_reward=False)
    test_agent(test_env, agent, visualize=False, test_episodes=1000, folder="2021_01_11_23_48_Crypto_trader", name="1772.66_Crypto_trader", comment="")
    test_agent(test_env, agent, visualize=False, test_episodes=1000, folder="2021_01_11_23_48_Crypto_trader", name="1377.86_Crypto_trader", comment="")

    agent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00001, epochs=1, optimizer=Adam, batch_size = 128, model="LSTM")
    test_env = CustomEnv(test_df, lookback_window_size=lookback_window_size, Show_reward=False)
    test_agent(test_env, agent, visualize=False, test_episodes=1000, folder="2021_01_11_23_43_Crypto_trader", name="1076.27_Crypto_trader", comment="")
```

[1000개 에피소드에 대한 가장 간단한 고밀도 네트워크는 이전 튜토리얼](https://pylessons.com/RL-BTC-BOT-NN/) 에서 평균 순자산 1043.40$를 기록했습니다 . 그것이 우리가 이기고 싶은 점수입니다.

## 밀집한

먼저 **Dense 네트워크** 를 훈련하고 테스트한  결과 다음과 같은 결과를 얻었습니다.

```python
Model name: 1277.39_Crypto_trader
net worth: 1054.483903083776
orders per episode: 140.566
no profit episodes: 14
```

[이전 튜토리얼](https://pylessons.com/RL-BTC-BOT-NN/) 에서 볼 수 있듯이 우리 모델은 에피소드당 주문 수와 1000개 에피소드를 통해 순자산이 마이너스로 완료된 주문 수를 측정하지 않았습니다. 어쨌든, 우리는 새로운 보상 전략을 사용한 Dense 현재 모델이 조금 더 나은 것을 볼 수 있습니다! 그러나 이 메트릭은 새로운 "무이익 에피소드" 메트릭만큼 중요하지 않습니다. 왜냐하면 수익은 더 낮지만 우리 모델이 돈을 잃지 않을 것이라는 확신이 있기 때문입니다. 따라서 "무수익 에피소드" 측정항목과 함께 "순자산"을 평가하는 것이 가장 좋습니다.

## CNN

**다음으로 CNN( Convolution Neural Networks** ) 모델을 훈련하고 테스트했습니다.  CNN에 대한 시계열 기사를 많이 볼 수 있지만 이것은 이야기할 주제가 아닙니다. 따라서 가장 좋은 평균 보상을 가진 저장된 모델을 선택하고 결과를 살펴보겠습니다.

```python
Model name: 1772.66_Crypto_trader
net worth: 1008.5786807732876
orders per episode: 134.177
no profit episodes: 341
```

우리가 볼 수 있듯이 우리 모델은 훈련하는 동안만큼 잘 수행되지 않습니다. 우리는 그것에 약간의 과적합이 있습니다. 결과는 끔찍합니다. 주문의 34%가 마이너스 기말 잔액을 가지고 있었고 이익은 무작위 모델보다 훨씬 나빴습니다. 그래서 Tensorboard 그래프에 의존하여 과적합이 덜한 다른 CNN 모델을 테스트하기로 결정했습니다.

```python
Model name: 1124.03_Crypto_trader
net worth: 1034.3430652376387
orders per episode: 70.152
no profit episodes: 55
```

우리가 볼 수 있듯이 이 모델은 끝까지 훈련하는 것보다 훨씬 더 나은 성능을 보이지만 여전히 가장 간단한 Dense 네트워크가 테스트 데이터 세트를 사용하여 이 모델에 대해 승리합니다.

## LSTM

마지막으로 LSTM 네트워크를 훈련시켜보자는 생각이 들었습니다. 또한 시계열 데이터에 대한 데이터 세트가 생성됩니다. 잘 수행되어야 합니다.

```python
Model name: 1076.27_Crypto_trader
net worth: 1027.3897665208062
orders per episode: 323.233
no profit episodes: 303
```

LSTM 네트워크를 훈련하는 데 Dense 및 CNN 네트워크보다 약 3배 정도 시간이 걸렸기 때문에 너무 많은 시간을 낭비하고 끔찍한 결과를 받았을 때 낙담했습니다. 재미있는 일을 하고 싶다고 생각했지만 이제 우리가 가진 것이 있습니다.

# 결론:

이러한 서면 실험을 수행하고 비교하는 데 너무 오래 걸리기 때문에 기사를 중단하기로 결정했습니다. 하지만 새로운 전략으로 Dense 네트워크 수익성을 개선할 수 있게 되어 기쁩니다.

나는 CNN과 LSTM을 사용하여 표시된 시계열 데이터를 예측하는 것이 부적절하다고 서두르지 않을 것입니다. 제 생각에는 우리 모델이 모든 시장 기능을 적절하게 학습할 수 있을 정도로 훈련 데이터를 너무 적게 사용했습니다.

나는 그렇게 빨리 포기하지 않을 것입니다. 나는 여전히 우리의 신경망이 시장을 이길 수 있다고 생각하지만 그는 더 많은 데이터가 필요합니다. 그래서 저는 곧 적어도 두 개의 튜토리얼을 더 작성할 계획이 있습니다.

1. 우리는 시장 데이터에 지표를 구현하려고 노력할 것이므로 우리 모델은 배울 수 있는 더 많은 기능을 갖게 될 것입니다.
2. 모델을 훈련하는 데 너무 적은 훈련 데이터를 사용한다는 것은 매우 분명하므로 인터넷에서 더 많은 기록 데이터를 다운로드하는 스크립트를 작성할 것입니다.
3. 다중 처리를 사용하여 다중 교육 환경을 실행하여 교육 프로세스의 속도를 높이고 병렬로 사용할 수 있는 맞춤형 거래 환경의 사본을 작성할 것입니다.

읽어 주셔서 감사합니다! 아직 해야 할 일이 많이 있습니다. YouTube에서 내 비디오를 구독하고 좋아요를 누르고 이 튜토리얼을 공유하면 다른 튜토리얼에서 일광을 볼 때 알림을 받게 됩니다! 항상 그렇듯이 이 튜토리얼에서 제공하는 모든 코드는 내 [GitHub](https://github.com/pythonlessons/RL-Bitcoin-trading-bot) 페이지에서 찾을 수 있으며 무료로 사용할 수 있습니다! 다음 편에서 뵙겠습니다.

**이 모든 튜토리얼은 교육 목적을 위한 것이며 거래 조언으로 받아들여서는 안됩니다. 투자를 잃을 가능성이 있으므로 이 튜토리얼, 이전 또는 향후 튜토리얼에서 정의된 알고리즘이나 전략을 기반으로 거래하지 않는 것이 가장 좋습니다.**



---

# 자동화된 비트코인 거래 봇 #5에 기술 지표 통합

## 가장 인기 있는 몇 가지 기술 지표를 Bitcoin 거래 봇에 통합하여 자동화된 거래를 하는 동안 더 나은 결정을 내릴 수 있도록 합니다.

이 튜토리얼은 가장 인기 있는 몇 가지 기술 지표를 Bitcoin 거래 봇에 통합하여 시장에서 자동화된 거래를 하는 동안 더 나은 결정을 학습합니다.

이전 튜토리얼에서 우리는 이미 비트코인 거래를 위한 파이썬 사용자 정의 환경을 만들었습니다. 이를 위해 강화 학습 에이전트를 작성했습니다. 또한 세 가지 아키텍처(Dense, CNN, LSTM)를 테스트하고 성능, 교육 기간 및 수익성을 비교했습니다. 그래서 저는 Price Action에서 수익성 있는 거래를 하는 거래 봇을 만들 수 있다면 지표를 통합하여 지표를 사용하여 봇 정확도와 수익성을 개선할 수 있지 않을까 생각했습니다. 해보자! 기술적 또는 기본적 분석 없이 블라인드 거래를 하는 트레이더나 투자자는 없을 것이라고 생각했습니다. 거의 모든 사람이 기술 지표를 사용합니다.



우선, 널리 알려지고 사용되는 5가지 기술 지표를 데이터 세트에 추가할 것입니다. 기술 지표는 데이터 세트에 몇 가지 관련 정보를 추가해야 하며, 이는 예측 모델의 예측 정보로 잘 보완될 수 있습니다. 이 지표의 조합은 우리 모델이 다음에서 찾을 수 있는 실용적인 관찰의 즐거운 균형을 제공해야 합니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-indicators/indicators_chart.png)나는 위에서 주어진 각각의 기술 지표를 곧 다룰 것입니다. 이를 구현하기 위해  지표 배치를 계산하는 데 사용되는 이미 준비된 [Python 라이브러리 를 사용합니다. ](https://github.com/bukosabino/ta)RL Bitcoin 거래 에이전트와 함께 이러한 지표로 성공하면 향후 더 많은 지표를 시도할 것입니다.

## 이동 평균(MA)

MA 또는 '단순 이동 평균'(SMA)은 단기 시장 급등을 방해하지 않으면서 현재 시장 추세의 방향을 결정하는 데 익숙한 지표입니다. 이동 평균 지표는 특정 기간 동안 선택한 상품의 시장 포인트를 결합합니다. 추세선의 방향을 나타내기 위해 타임프레임 포인트의 수로 나눕니다.

사용되는 데이터는 MA의 길이에 따라 다릅니다. 예를 들어, 200 MA에는 200일의 기록 정보가 필요합니다. MA 지표를 활용하면 지지 및 저항 수준을 연구하고 이전 가격 움직임(시장의 역사)을 볼 수 있습니다. 이는 향후 가능한 가격 패턴을 결정할 수 있음을 의미합니다.



OHCL 막대와 Matplotlib로 지표를 그리는 데 사용할 다음 함수를 작성했습니다.



```python
import matplotlib.pyplot as plt
from mplfinance.original_flavor import candlestick_ohlc
import matplotlib.dates as mpl_dates

def Plot_OHCL(df):
    df_original = df.copy()
    # necessary convert to datetime
    df["Date"] = pd.to_datetime(df.Date)
    df["Date"] = df["Date"].apply(mpl_dates.date2num)

    df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]
    
    # We are using the style ‘ggplot’
    plt.style.use('ggplot')
    
    # figsize attribute allows us to specify the width and height of a figure in unit inches
    fig = plt.figure(figsize=(16,8)) 

    # Create top subplot for price axis
    ax1 = plt.subplot2grid((6,1), (0,0), rowspan=5, colspan=1)

    # Create bottom subplot for volume which shares its x-axis
    ax2 = plt.subplot2grid((6,1), (5,0), rowspan=1, colspan=1, sharex=ax1)

    candlestick_ohlc(ax1, df.values, width=0.8/24, colorup='green', colordown='red', alpha=0.8)
    ax1.set_ylabel('Price', fontsize=12)
    plt.xlabel('Date')
    plt.xticks(rotation=45)

    # Add Simple Moving Average
    ax1.plot(df["Date"], df_original['sma7'],'-')
    ax1.plot(df["Date"], df_original['sma25'],'-')
    ax1.plot(df["Date"], df_original['sma99'],'-')

    # Add Bollinger Bands
    ax1.plot(df["Date"], df_original['bb_bbm'],'-')
    ax1.plot(df["Date"], df_original['bb_bbh'],'-')
    ax1.plot(df["Date"], df_original['bb_bbl'],'-')

    # Add Parabolic Stop and Reverse
    ax1.plot(df["Date"], df_original['psar'],'.')

    # # Add Moving Average Convergence Divergence
    ax2.plot(df["Date"], df_original['MACD'],'-')

    # # Add Relative Strength Index
    ax2.plot(df["Date"], df_original['RSI'],'-')

    # beautify the x-labels (Our Date format)
    ax1.xaxis.set_major_formatter(mpl_dates.DateFormatter('%y-%m-%d'))# %H:%M:%S'))
    fig.autofmt_xdate()
    fig.tight_layout()
    
    plt.show()
```



[두 번째 튜토리얼](https://pylessons.com/RL-BTC-BOT-visualization/) 에서 이미 유사한 기능을 작성했기 때문에 이 코드를 한 줄씩 설명하지 않겠습니다 . 여기에서 모든 것을 단계별로 설명했습니다.

3개의 SMA 표시기를 모두 데이터 프레임에 추가하고 다음과 같은 간단한 코드로 플롯할 수 있습니다.

```python
import pandas as pd
from ta.trend import SMAIndicator

def AddIndicators(df):
    # Add Simple Moving Average (SMA) indicators
    df["sma7"] = SMAIndicator(close=df["Close"], window=7, fillna=True).sma_indicator()
    df["sma25"] = SMAIndicator(close=df["Close"], window=25, fillna=True).sma_indicator()
    df["sma99"] = SMAIndicator(close=df["Close"], window=99, fillna=True).sma_indicator()
    
    return df

if __name__ == "__main__":   
    df = pd.read_csv('./pricedata.csv')
    df = df.sort_values('Date')
    df = AddIndicators(df)

    test_df = df[-400:]

    # Add Simple Moving Average
    Plot_OHCL(test_df, ax1_indicators=["sma7", "sma25", "sma99"])
```

전체 데이터 세트에 대한 지표 계산 후 마지막 720개 막대에 대해 플롯하면 다음과 같습니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-indicators/SMA.png)7, 25, 99 창 크기의 SMA 표시기 3개

단순 이동 평균에 대해서는 더 이상 설명할 것이 없습니다. 인터넷에는 많은 정보가 있습니다.

## 볼린저 밴드

볼린저 밴드는 시장 가치의 단순 이동 평균(SMA)에서 멀리 떨어진 두 개의 표준 편차(양수 및 음수)로 계산된 추세선 그룹으로 요약된 기술 분석 도구로, 사용자 선호도에 따라 조정될 수 있습니다. Bollinger Bands는 저명한 테크니컬 데이 트레이더인 John Bollinger에 의해 개발되고 저작권이 지정되었으며 투자자들이 시장 상황(과매도 또는 과매수)을 정확하게 식별할 수 있는 더 나은 가능성을 제공할 수 있는 기회를 얻도록 설계되었습니다. 볼린저 밴드는 현대 기술입니다. 많은 거래자들은 가격이 상위 밴드에 가까울수록 시장이 더 과매수되고 가격이 하위 밴드에 가까울수록 시장이 더 많이 과매도되었다고 믿습니다. SMA에서 했던 것과 동일한 데이터 세트에 대한 코드에 이것을 추가해 보겠습니다.

```python
import pandas as pd
from ta.volatility import BollingerBands

def AddIndicators(df):
    # Add Bollinger Bands indicator
    indicator_bb = BollingerBands(close=df["Close"], window=20, window_dev=2)
    df['bb_bbm'] = indicator_bb.bollinger_mavg()
    df['bb_bbh'] = indicator_bb.bollinger_hband()
    df['bb_bbl'] = indicator_bb.bollinger_lband()
    
    return df

if __name__ == "__main__":   
    df = pd.read_csv('./pricedata.csv')
    df = df.sort_values('Date')
    df = AddIndicators(df)

    test_df = df[-400:]

    # Add Bollinger Bands
    Plot_OHCL(test_df, ax1_indicators=["bb_bbm", "bb_bbh", "bb_bbl"])
```

다음 결과를 받게 됩니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-indicators/Bollinger_Bands.png)창 크기가 20인 볼린저 밴드

이 지표에 따라 시장 거래를 하는 것에 대한 어떠한 조언도 하지 않겠습니다. 나는 그것들을 다룰 것이고 모든 노력은 나의 Reinforcement Learning 에이전트에게 맡길 것입니다.

#### 포물선 정지 및 역회전(포물선 SAR)

포물선형 SAR은 시장의 방향을 결정하는 데 널리 사용되는 기술 지표이지만 시장의 방향이 바뀌면 정확한 순간에 주목하게 됩니다. 이 표시기는 J. Welles Wilder Junior가 개발한 포물선 SAR인 "정지 및 반전 시스템"이라고도 합니다. - 상대 강도 지수(RSI)의 작성자.

표시기는 차트의 촛대 막대보다 높거나 낮은 일련의 점처럼 보입니다. 점이 뒤집히면 자산 방향이 변경될 수 있음을 나타냅니다. 예를 들어 점이 촛대 가격 위에 있었다가 가격 아래에 나타나면 시장 추세의 변화를 알릴 수 있습니다. 촛대 아래로 떨어지면 낙관적 인 강세 신호로 간주됩니다. 반대로 수수료 위의 점은 약세가 통제권에 있고 모멘텀이 계속 하향세를 유지할 가능성이 있음을 나타냅니다.



SAR 점은 점들이 시장 가격을 따라잡을 때까지 시장 방향이 올라감에 따라 조금 더 빠르게 움직이기 시작합니다. 시장 가격이 상승함에 따라 점도 상승할 것입니다. 처음에는 천천히 그리고 나서 속도를 높이고 추세에 따라 가속화됩니다. 다음 코드를 사용하여 차트에 PSAR을 추가할 수 있습니다.

```python
import pandas as pd
from ta.trend import PSARIndicator

def AddIndicators(df):
    # Add Parabolic Stop and Reverse (Parabolic SAR) indicator
    indicator_psar = PSARIndicator(high=df["High"], low=df["Low"], close=df["Close"], step=0.02, max_step=2, fillna=True)
    df['psar'] = indicator_psar.psar()
    
    return df

if __name__ == "__main__":   
    df = pd.read_csv('./pricedata.csv')
    df = df.sort_values('Date')
    df = AddIndicators(df)

    test_df = df[-400:]

    # Add Parabolic Stop and Reverse
    Plot_OHCL(test_df, ax1_indicators=["psar"])
```

그러나 이 표시기를 한 줄로 표시하는 대신 점으로 표시하려면 Plot_OHCL 함수에서 다음과 같이 변경해야 합니다.

```python
# plot all ax1 indicators
for indicator in ax1_indicators:
    ax1.plot(df["Date"], df_original[indicator],'-')
```

다음으로:

```python
# plot all ax1 indicators
for indicator in ax1_indicators:
    ax1.plot(df["Date"], df_original[indicator],'.')
```

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-indicators/PSAR.png)포물선 SAR 표시기

위의 차트는 지표가 추세 동안 이익을 포착하는 데 잘 작동하지만 가격이 옆으로 움직이거나 고르지 않은 시장에서 거래될 때 많은 잘못된 신호로 이어질 수 있음을 보여줍니다. 지표는 가격이 상승하는 동안 열린 위치에서 주문을 유지하는 것이 가장 좋은 아이디어임을 보여줍니다. 시장이 횡보하기 시작하면 투자자는 추가 손실 및/또는 작은 이익을 기대해야 합니다.

#### 이동 평균 수렴 발산(MACD)

MACD(이동 평균 수렴 다이버전스)는 시장 가격의 두 이동 평균 간의 상관 관계를 보여주는 추세 추종 모멘텀 지표입니다. 기본 MACD는 12기간 EMA에서 26기간 지수 이동 평균(EMA)을 빼서 계산됩니다.

이 계산의 결과로 MACD 라인을 받습니다. MACD의 9일 EMA는 매수 및 매도 신호의 트리거로 작동할 수 있는 MACD 라인 상단에 그려진 "신호 라인"입니다. 거래자는 MACD가 신호선을 상향 돌파하면 매수 주문을 하고 MACD가 신호선을 하향 돌파하면 증권을 매도(또는 공매도)할 수 있습니다. MACD(이동 평균 수렴 발산) 지표는 종종 여러 방식으로 해석됩니다. 그러나 가장 일반적인 방법은 분기, 교차 및 빠른 상승/하강입니다.

- MACD는 12기간 EMA에서 26기간 지수이동평균(EMA)을 빼서 계산합니다.
- MACD는 신호선 위(매수) 또는 아래(매도)를 교차하면 기술 신호를 트리거합니다.
- 교차 속도는 추가로 시장이 과매수 또는 과매도 상태임을 나타내는 것으로 간주됩니다.
- MACD는 투자자가 가격 내에서 낙관적 또는 비관적 움직임이 강화되고 있는지 또는 약화되고 있는지 인지하는 데 도움이 됩니다.

시장 주문을 수행하는 동안 MACD가 우리를 도울 수 있는 방법은 많이 있습니다. 위에서 언급한 지표와 달리 MACD는 계산되는 동안 다른 범위를 가지고 있기 때문에 볼륨 하위 그림에 그려지며 가격 하위 그림에는 그릴 수 없습니다. 다음 코드를 사용합니다.

```python
import pandas as pd
from ta.trend import macd

def AddIndicators(df):
    # Add Moving Average Convergence Divergence (MACD) indicator
    df["MACD"] = macd(close=df["Close"], window_slow=26, window_fast=12, fillna=True)
    
    return df

if __name__ == "__main__":   
    df = pd.read_csv('./pricedata.csv')
    df = df.sort_values('Date')
    df = AddIndicators(df)

    test_df = df[-400:]

    # Add Moving Average Convergence Divergence
    Plot_OHCL(test_df, ax2_indicators=["MACD"])
```



이 표시기는 이전처럼 정보를 제공하지 않지만 신경망은 스스로 알아낼 것입니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-indicators/MACD.png)창 크기가 26 및 12인 MACD 표시기

## 상대 강도 지수(RSI)

상대 강도 지수(RSI)는 현재 시장 가격 내에서 과매수 또는 과매도 조건을 추정하기 위해 최근 시장 변화의 규모를 측정하는 기술 분석에 사용되는 모멘텀 지표입니다. RSI는 오실레이터(2개의 극단 사이를 이동하는 선 그래프)로 표시되며 0-100 사이를 읽을 수 있습니다. 지표는 원래 J. Welles Wilder Junior가 개발했습니다. 1978년 그의 저서 "기술적 거래 시스템의 새로운 개념"에서 소개되었습니다. 값이 70보다 크면 증권이 과매수 또는 과대 평가되고 있으며 추세 반전 또는 가치 하락을 위해 설정될 것이라는 RSI 측정의 전통적인 해석 및 사용. 판독값 30<은 과매도 또는 저평가 상태를 나타냅니다.

- RSI는 1978년에 개발된 바이러스로 알려진 모멘텀 오실레이터 지표입니다.
- RSI는 낙관적 및 비관적 시장 모멘텀에 관한 기술 거래자 신호를 제공하며 종종 가격 그래프의 낮은 위치에 표시됩니다.
- 자산은 일반적으로 RSI가 70% 이상이고 30% 미만이면 과매수 상태로 간주됩니다.

이것은 우리가 거래자와 함께 사용할 마지막 지표이며 이전과 마찬가지로 이 지표를 그리는 것도 간단합니다.

```python
import pandas as pd
from ta.momentum import rsi

def AddIndicators(df):
    # Add Relative Strength Index (RSI) indicator
    df["RSI"] = rsi(close=df["Close"], window=14, fillna=True)
    
    return df

if __name__ == "__main__":   
    df = pd.read_csv('./pricedata.csv')
    df = df.sort_values('Date')
    df = AddIndicators(df)

    test_df = df[-400:]

    # Add Relative Strength Index
    Plot_OHCL(test_df, ax2_indicators=["RSI"])
```

위의 코드는 다음과 같은 결과를 제공합니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-indicators/RSI.png)창 크기가 14인 RSI 표시기

위 차트에서 볼 수 있듯이 RSI 지표는 주식이 상승 추세에 있는 동안 장기간 과매수(> 70%) 영역에 머무를 수 있습니다. 이 지표는 주식이 하락세에 있을 때 과매도(< 30%) 영역에 오랫동안 머물 수도 있습니다.

이것은 이 다섯 가지 지표에 대한 간략한 소개이며, 그것에 대해 말할 수 있는 것이 훨씬 더 많지만 우리가 이러한 거래를 하지 않을 것이기 때문에 이에 대해 확장하고 싶지 않습니다. 우리는 강화 학습 에이전트가 수익성 있는 작업을 수행하도록 가르치고 싶습니다.



지금까지 다섯 가지 지표를 하나씩 다루었지만 이 지표를 따로 사용하지는 않을 것입니다. 이러한 지표를 시장 정보 및 주문 정보와 함께 처리합니다. 이 모든 데이터를 Bitcoin Reinforcement Learning 에이전트에 보내 어떤 조치를 취해야 하는지 결정합니다. 하지만 그렇게 하기 전에 지표가 모두 하나의 차트에 그려졌을 때 지표가 어떻게 보이는지 살펴보겠습니다.

이 그래프를 플로팅하기 위해 `Plot_OHCL`에 삽입한 이라는 새 함수를 작성 `utils.py`했지만 기본 플로팅을 위해 `indicators.py`OHCL 막대를 사용하여 모든 지표를 하나의 플롯으로 플롯하는 데 사용할 수 있는 또 다른 스크립트를 작성했습니다.

이것은 스크립트 플로팅 기능 `Plot_OHCL`에서 우리의 기능입니다:`utils.py`

```python
import matplotlib.pyplot as plt
from mplfinance.original_flavor import candlestick_ohlc
import matplotlib.dates as mpl_dates

def Plot_OHCL(df):
    df_original = df.copy()
    # necessary convert to datetime
    df["Date"] = pd.to_datetime(df.Date)
    df["Date"] = df["Date"].apply(mpl_dates.date2num)

    df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]
    
    # We are using the style ‘ggplot’
    plt.style.use('ggplot')
    
    # figsize attribute allows us to specify the width and height of a figure in unit inches
    fig = plt.figure(figsize=(16,8)) 

    # Create top subplot for price axis
    ax1 = plt.subplot2grid((6,1), (0,0), rowspan=5, colspan=1)

    # Create bottom subplot for volume which shares its x-axis
    ax2 = plt.subplot2grid((6,1), (5,0), rowspan=1, colspan=1, sharex=ax1)

    candlestick_ohlc(ax1, df.values, width=0.8/24, colorup='green', colordown='red', alpha=0.8)
    ax1.set_ylabel('Price', fontsize=12)
    plt.xlabel('Date')
    plt.xticks(rotation=45)

    # Add Simple Moving Average
    ax1.plot(df["Date"], df_original['sma7'],'-')
    ax1.plot(df["Date"], df_original['sma25'],'-')
    ax1.plot(df["Date"], df_original['sma99'],'-')

    # Add Bollinger Bands
    ax1.plot(df["Date"], df_original['bb_bbm'],'-')
    ax1.plot(df["Date"], df_original['bb_bbh'],'-')
    ax1.plot(df["Date"], df_original['bb_bbl'],'-')

    # Add Parabolic Stop and Reverse
    ax1.plot(df["Date"], df_original['psar'],'.')

    # # Add Moving Average Convergence Divergence
    ax2.plot(df["Date"], df_original['MACD'],'-')

    # # Add Relative Strength Index
    ax2.plot(df["Date"], df_original['RSI'],'-')

    # beautify the x-labels (Our Date format)
    ax1.xaxis.set_major_formatter(mpl_dates.DateFormatter('%y-%m-%d'))# %H:%M:%S'))
    fig.autofmt_xdate()
    fig.tight_layout()
    
    plt.show()
```

그리고 이것은 지표를 표시하는 주요 코드입니다:

```python
import pandas as pd
from ta.trend import SMAIndicator, macd, PSARIndicator
from ta.volatility import BollingerBands
from ta.momentum import rsi
from utils import Plot_OHCL

def AddIndicators(df):
    # Add Simple Moving Average (SMA) indicators
    df["sma7"] = SMAIndicator(close=df["Close"], window=7, fillna=True).sma_indicator()
    df["sma25"] = SMAIndicator(close=df["Close"], window=25, fillna=True).sma_indicator()
    df["sma99"] = SMAIndicator(close=df["Close"], window=99, fillna=True).sma_indicator()
    
    # Add Bollinger Bands indicator
    indicator_bb = BollingerBands(close=df["Close"], window=20, window_dev=2)
    df['bb_bbm'] = indicator_bb.bollinger_mavg()
    df['bb_bbh'] = indicator_bb.bollinger_hband()
    df['bb_bbl'] = indicator_bb.bollinger_lband()

    # Add Parabolic Stop and Reverse (Parabolic SAR) indicator
    indicator_psar = PSARIndicator(high=df["High"], low=df["Low"], close=df["Close"], step=0.02, max_step=2, fillna=True)
    df['psar'] = indicator_psar.psar()

    # Add Moving Average Convergence Divergence (MACD) indicator
    df["MACD"] = macd(close=df["Close"], window_slow=26, window_fast=12, fillna=True) # mazas

    # Add Relative Strength Index (RSI) indicator
    df["RSI"] = rsi(close=df["Close"], window=14, fillna=True) # mazas
    
    return df

if __name__ == "__main__":   
    df = pd.read_csv('./pricedata.csv')
    df = df.sort_values('Date')
    df = AddIndicators(df)

    test_df = df[-400:]

    Plot_OHCL(test_df)
```

오류 없이 실행되면 다음 차트가 표시되어야 합니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-indicators/Indicators.png)하나의 차트에 5가지 기술 지표

이제 우리가 다룬 모든 지표를 하나의 차트로 결합하면서 결과를 볼 수 있습니다. 지표와 가격변동 간의 상관관계를 발견할 수 있다고 가정할 수 있다. 봇이 이전 자습서보다 더 많은 수익을 올린다면 이러한 지표가 사용하는 데 도움이 된다고 가정할 수 있습니다. 그러나 경험이 풍부한 거래자가 아니라면 장기적으로 여전히 돈을 잃을 것입니다. 따라서 우리는 이 기술 정보를 신경망으로 보내 목적이 있는 상관 관계를 찾습니다.

# RL 에이전트 코드에 지표 구현

우리는 이미 이러한 지표를 데이터세트에 구현하는 방법과 시각화하는 방법을 다루었습니다. Open, High, Close 등의 숫자로 모든 동일한 단계를 수행합니다. 그래서 우리는 9개의 매개변수가 있는 5개의 지표를 간략하게 다루었습니다. 3개는 SMA, 3개는 볼린저 밴드, 그리고 각각 PSAR, MACD, RSI 지표에 의해 표시됩니다. 이것은 10개의 입력 기능이 있기 전에 모델 입력 상태를 수정해야 함을 의미합니다. 지금은 10+9개의 기능이 있습니다.

```python
self.state_size = (lookback_window_size, 10)
we change to ->
self.state_size = (lookback_window_size, 10+9)
```

이러한 지표는 이미 계산되어 데이터 프레임에 삽입되기 때문에 사용하기가 매우 간단합니다. 주로 클래스 에서 `reset`및 `_next_observation`함수 를 수정해야 합니다.`CustomEnv`

그 `__init__`자리에서 우리는 새로운 deque 목록을 정의합니다:

```
self.indicators_history = deque(maxlen=self.lookback_window_size)
```

모든 단계의 지표 정보를 저장할 위치입니다. 정보 만 처리 `self.market_history`한 다음 와 연결 하기 전에는 전체 프로세스 `self.orders_history`에 추가해야 합니다 .`self.indicators_history`

`reset`함수를 다음과 같이 수정합니다 .

```python
for i in reversed(range(self.lookback_window_size)):
    current_step = self.current_step - i
    self.orders_history.append([self.balance, self.net_worth, self.crypto_bought, self.crypto_sold, self.crypto_held])

    self.market_history.append([self.df.loc[current_step, 'Open'],
                                self.df.loc[current_step, 'High'],
                                self.df.loc[current_step, 'Low'],
                                self.df.loc[current_step, 'Close'],
                                self.df.loc[current_step, 'Volume'],
                                ])

    self.indicators_history.append(
        [self.df.loc[current_step, 'sma7'] / self.normalize_value,
                                self.df.loc[current_step, 'sma25'] / self.normalize_value,
                                self.df.loc[current_step, 'sma99'] / self.normalize_value,
                                self.df.loc[current_step, 'bb_bbm'] / self.normalize_value,
                                self.df.loc[current_step, 'bb_bbh'] / self.normalize_value,
                                self.df.loc[current_step, 'bb_bbl'] / self.normalize_value,
                                self.df.loc[current_step, 'psar'] / self.normalize_value,
                                self.df.loc[current_step, 'MACD'] / 400,
                                self.df.loc[current_step, 'RSI'] / 100
                                ])

state = np.concatenate((self.market_history, self.orders_history), axis=1) / self.normalize_value
state = np.concatenate((state, self.indicators_history), axis=1)

return state
```

또한 `_next_observation`함수를 수정해야 합니다.

```python
# Get the data points for the given current_step
def _next_observation(self):
    self.market_history.append([self.df.loc[self.current_step, 'Open'],
                                self.df.loc[self.current_step, 'High'],
                                self.df.loc[self.current_step, 'Low'],
                                self.df.loc[self.current_step, 'Close'],
                                self.df.loc[self.current_step, 'Volume'],
                                ])

    self.indicators_history.append([self.df.loc[self.current_step, 'sma7'] / self.normalize_value,
                                self.df.loc[self.current_step, 'sma25'] / self.normalize_value,
                                self.df.loc[self.current_step, 'sma99'] / self.normalize_value,
                                self.df.loc[self.current_step, 'bb_bbm'] / self.normalize_value,
                                self.df.loc[self.current_step, 'bb_bbh'] / self.normalize_value,
                                self.df.loc[self.current_step, 'bb_bbl'] / self.normalize_value,
                                self.df.loc[self.current_step, 'psar'] / self.normalize_value,
                                self.df.loc[self.current_step, 'MACD'] / 400,
                                self.df.loc[self.current_step, 'RSI'] / 100
                                ])

    obs = np.concatenate((self.market_history, self.orders_history), axis=1) / self.normalize_value
    obs = np.concatenate((obs, self.indicators_history), axis=1)

return obs
```

이 코드를 보는 동안 한 가지 질문이 제기될 것입니다. `self.normalize_value`내가 400과 100으로 나누는 대신 두 줄로 나누는 이유는 설명하는 것보다 시각적으로 답을 보여주는 것이 더 간단합니다.

전체 데이터 세트에 대해 스크립트를 실행 `indicators.py`하면 다음 그래프가 표시됩니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-indicators/Indicators_fulldf%20(1).png)

그래프에 많은 시간 프레임이 포함되어 있기 때문에 상당히 압축된 것처럼 보이지만 하단 서브플롯을 보면 MACD 곡선의 최대값과 최소값이 +-300 사이이고 RSI가 0–100 사이에서 변동하는 것을 볼 수 있습니다. 저는 이 값을 각각 400과 100으로 나누어 정규화하기로 했습니다.

# 모델 학습 및 테스트

최고의 모델 아키텍처(Dense, CNN 또는 LSTM)를 테스트한 이전 자습서와 달리 이제 이러한 모든 모델 간에 테스트할 필요가 없습니다. 동일한 매개변수와 동일한 양의 훈련 단계에 대해 동일한 Dense 모델을 훈련할 것입니다. 훈련 후에 우리는 이 모델을 실행하는 동안 얻은 결과를 보이지 않는 동일한 테스트 데이터 세트와 비교할 것입니다.

```python
from indicators import AddIndicators
if __name__ == "__main__":            
    df = pd.read_csv('./pricedata.csv')
    df = df.sort_values('Date')
    df = AddIndicators(df) # insert indicators to df

    lookback_window_size = 50
    test_window = 720 # 30 days 
    train_df = df[:-test_window-lookback_window_size]
    test_df = df[-test_window-lookback_window_size:]

    agent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00001, epochs=5, optimizer=Adam, batch_size = 32, model="Dense")
    train_env = CustomEnv(train_df, lookback_window_size=lookback_window_size)
    train_agent(train_env, agent, visualize=False, train_episodes=50000, training_batch_size=500)
    
    #test_env = CustomEnv(test_df, lookback_window_size=lookback_window_size, Show_reward=False, Show_indicators=False)
    #test_agent(test_env, agent, visualize=False, test_episodes=10, folder="", name="", comment="")
```

보시다시피 이 코드 부분은 변경되지 않습니다. `AddIndicators`전체 데이터 세트를 처리하기 위해 코드의 5번째 줄에서 호출하는 함수 를 가져와야 한다는 점을 제외하고 다른 교육 단계는 모두 동일합니다.

다음 매개변수를 사용하여 모델을 훈련했습니다.

```python
training start: 2021-01-18 22:18
initial_balance: 1000
training episodes: 50000
lookback_window_size: 50
lr: 1e-05
epochs: 5
batch size: 32
normalize_value: 40000
model: Dense
training end: 2021-01-19 14:20
```

보시다시피 매개변수는 이전 자습서 시리즈와 거의 동일하므로 교육에 약 16시간이 걸렸습니다. 그러나 실제로 우리는 훈련 부분이 아니라 테스트에 관한 것이므로 훈련된 모델을 테스트하기 위해 다음 코드를 실행했습니다.

```python
if __name__ == "__main__":            
    df = pd.read_csv('./pricedata.csv')
    df = df.sort_values('Date')
    df = AddIndicators(df) # insert indicators to df

    lookback_window_size = 50
    test_window = 720 # 30 days 
    train_df = df[:-test_window-lookback_window_size]
    test_df = df[-test_window-lookback_window_size:]

    agent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00001, epochs=5, optimizer=Adam, batch_size = 32, model="Dense")
    test_env = CustomEnv(test_df, lookback_window_size=lookback_window_size, Show_reward=False, Show_indicators=False)
    test_agent(test_env, agent, visualize=False, test_episodes=1000, folder="2021_01_18_22_18_Crypto_trader", name="1933.71_Crypto_trader", comment="")
```

위의 코드를 실행하여 보이지 않는 테스트 데이터 세트에 대해 Bitcoin 거래 봇을 테스트한 후 꽤 만족스러운 결과를 얻었습니다.

```python
test episodes: 1000
net worth: 1078.6216040753616,
orders per episode: 135.945
no profit episodes: 1
comment: Dense network
```

이 결과를 이전 자습서와 비교하면 다음 결과가 나타납니다.

```python
net worth: 1054.483903083776
orders per episode: 140.566
no profit episodes: 14
```

우리는 더 나은 결과를 볼 수 있습니다. 봇이 에피소드당 더 적은 주문을 하는 것은 중요하지 않지만 이전보다 2% 더 많은 수익을 내고 가장 중요한 것은 마이너스 순자산으로 끝난 에피소드가 단 하나뿐이라는 것입니다.

요약하면, 우리 봇은 완벽한 조건에서 Bitcoin을 거래하는 동안 보이지 않는 데이터에서 거의 8%의 이익을 얻었습니다. 환상적인 결과입니다!

봇이 모든 지표 등으로 어떤 주문을 하는지 보려면 다음 `Show_reward=True, Show_indicators=True, visualize=True`매개변수를 변경해야 하며 동일한 테스트 코드를 실행하는 동안 비슷한 결과를 볼 수 있습니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-indicators/gameplay.gif)

# 결론:

현재 Bitcoin 거래 에이전트가 우리가 달성할 수 있는 몇 가지 긍정적인 결과를 보여주고 있음을 알 수 있습니다. 물론 이러한 에이전트의 성능을 개선하기 위해 할 수 있는 일이 여전히 많이 있습니다. 그러나 이러한 변경 사항을 개발하고 실험하기 위해서는 많은 노력과 시간이 필요합니다. 이 주제에 대한 모든 새 자습서는 점점 더 많은 시간이 소요되며 내가 할 수 있는 일은 없습니다. 유일한 동기는 긍정적인 결과를 보는 것이며, 비트코인을 수익성 있게 거래하면서 인공 지능을 사용할 수 있음을 증명했습니다! 이 튜토리얼과 과거에 있었던 이전 튜토리얼을 포함하여 우리는 많은 도전을 물리쳤습니다.

이 시점까지 이전의 모든 자습서에서 동일한 기록 데이터 내에서 에이전트를 교육하고 테스트했습니다. 그럼에도 불구하고 이 봇이 더 많은 교육 데이터와 현재 비트코인 가격으로 어떻게 수행될지 모두가 관심이 있다고 생각합니다. 그래서 이것을 해결하기 위해 다음 튜토리얼에서는 Cryptocurrency 시장에서 과거 데이터를 다운로드하고 교육을 수행하는 방법에 대해 작성했습니다!

훨씬 더 많은 훈련 데이터가 있기 때문에 훈련을 훈련하는 데 며칠이 걸릴 수 있는 문제에 직면할 수 있습니다. 따라서 이 문제를 해결하여 한 번에 여러 시뮬레이션된 거래 환경(예: 16개 환경)을 실행하여 교육하는 동안 더 적은 시간을 보낼 수 있도록 노력하겠습니다. 자세한 내용은 다음 튜토리얼에서 뵙겠습니다!

읽어 주셔서 감사합니다! 아직 해야 할 일이 많이 있습니다. YouTube에서 내 비디오를 구독하고 좋아요를 누르고 이 튜토리얼을 공유하면 다른 튜토리얼에서 일광을 볼 때 알림을 받게 됩니다! 항상 그렇듯이 이 튜토리얼에서 제공하는 모든 코드는 내 [GitHub](https://github.com/pythonlessons/RL-Bitcoin-trading-bot) 페이지에서 찾을 수 있으며 무료로 사용할 수 있습니다! 다음 편에서 뵙겠습니다.

**이 모든 튜토리얼은 교육 목적을 위한 것이며 거래 조언으로 받아들여서는 안됩니다. 투자를 잃을 가능성이 있으므로 이 튜토리얼, 이전 또는 향후 튜토리얼에서 정의된 알고리즘이나 전략을 기반으로 거래하지 않는 것이 가장 좋습니다.**



---

# 과거 암호화폐 데이터를 사용하는 비트코인 거래 봇 #6

## 거래소 API를 통해 과거 암호화폐 OHCL 시장 데이터를 다운로드하는 가장 간단한 방법을 배웁니다. 우리는 이 데이터를 사용하여 마침내 시장을 이길 수 있는 RL Bitcoin 거래 에이전트를 훈련할 것입니다!

이 튜토리얼은 거래소 API를 통해 과거 암호화폐 OHCL 시장 데이터를 다운로드하는 가장 간단한 방법을 안내합니다. 우리는 이 데이터를 사용하여 마침내 시장을 이길 수 있는 강화 학습 Bitcoin 거래 에이전트를 훈련할 것입니다!

알고리즘 거래는 암호화폐 시장의 급변하고 변동성이 큰 환경을 해결하기 위해 널리 사용되는 방법입니다. 그러나 자동화된 거래 전략을 구현하는 것은 어렵고 많은 과거 데이터와 계산 능력을 포함하는 많은 백테스팅이 필요합니다. Bitcoin RL 거래 봇을 개발하는 동안 더 낮은 기간의 과거 기간 데이터를 얻는 것이 매우 어렵다는 것을 알게 되었습니다. 시장에는 과거 암호화폐 데이터를 제공하는 소스가 많이 있지만 대부분은 단점이 있습니다. 그들 중 많은 사람들이 그러한 데이터에 대해 비용을 지불해야 하며 그렇게 저렴하지 않습니다. 무료 데이터 소스는 불충분한 시간 해상도 데이터(일일)만을 제공하거나 제한된 양의 통화 쌍의 제한된 기간을 포함합니다. *이 튜토리얼은 과거 시가, 고가, 저가, 종가 데이터 (OHLC)* 를 얻는 방법을 보여줍니다. 1분 또는 필요한 모든 해결 방법은 돈을 들이지 않고 몇 줄의 Python 코드로 수행할 수 있는 마법 같은 작업이 아닙니다.

## API 설치

이 튜토리얼에서는 [Bitfinex exchange API](https://docs.bitfinex.com/docs/open-source-libraries) 를 사용하여  과거 OHCL 데이터를 다운로드하는 방법을 보여줍니다. 그러나 이 접근 방식은 유사한 API를 제공하는 다른 거래소에서도 작동하지만 지금은 Bitfinex를 선택했습니다. 또한 Bitfinex 계정이 없더라도 걱정하지 마십시오. 공개 API 끝점만 사용하기 때문에 이 작업 없이 수행할 수 있습니다. [API가 무엇인지, 작동 방식 또는 사용 방법에 익숙하지 않은 경우 Bitfinex API 설명서](https://docs.bitfinex.com/docs) 에서 이에 대해 읽을 수 있습니다 . 이것은 또한 우리 알고리즘이 교환과 상호 작용할 인터페이스입니다. 그러나 이미 여러 가지 구현이 가능하므로 걱정하지 마십시오. 통신을 위해 Python 인터페이스를 작성할 필요가 없습니다. 이를 위해 `bitfinex-tencars`pip를 통해 설치할 수 있는 python 라이브러리를 사용할 것입니다.`ip install bitfinex-tencars`

꽤 오래되었지만 우리에게 작동하는 동안에는 중요하지 않습니다. 또한 [bitfinex-api-py](https://github.com/bitfinexcom/bitfinex-api-py) 와 같은 다른 라이브러리 를 사용하여 실시간 거래를 할 수 있습니다. `bitfinex-tencars`그래도 이 튜토리얼은 라이브 Python 거래에 관한 것이 아니기 때문에 패키지를 사용하는 것이 좋습니다 .

## API 클라이언트 사용

Bitfinex API 문서를 보면 v1과 v2라는 두 가지 API 버전이 모두 방금 설치한 클라이언트에서 구현되었지만 저는 v2 API만 사용하기로 결정했습니다. Bitfinex API 클라이언트 파이썬 라이브러리를 가져온 후 아래 코드를 실행하여 v2 API 인스턴스를 생성해야 합니다. 여기서는 키를 제공하지 않으므로 공개 엔드포인트에만 액세스할 수 있다는 점에 유의하십시오. 이는 Bitfinex 계정에서 어떻게든 잔액을 잃을 위험이 없음을 의미합니다. 물론 코드를 실행한 후에 해당 메시지가 표시됩니다.

```python
import bitfinex
# Create api instance of the v2 API
api_v2 = bitfinex.bitfinex_v2.api_v2()
```



이것이 우리가 과거 시장 데이터에 대한 문을 여는 방법입니다. Bitfinex 문서에서 우리는 공개 끝점 중 하나가 양초라고 하는 것을 보았습니다. 이는  모든 거래소 의 [촛대 차트 뒤에 있는 데이터를 반환합니다. ](https://en.wikipedia.org/wiki/Candlestick_chart)이러한 종류의 데이터에는 타임스탬프, 시가, 종가, 고가 및 저가, 일반적으로 거래량이 포함됩니다. 처음에는 기본 설정으로 호출하여 클라이언트를 통해 이 끝점과 상호 작용하는 가장 간단한 방법을 시도합니다.

```python
result = api_v2.candles()
print(len(result))
```

위의 라인은 BTC/USD 가격에 대한 OHLC 데이터의 마지막 1000분을 제공합니다. 좋은 성과지만 우리는 일반적으로 오래 전의 기간이나 다른 통화 쌍에 관심이 있습니다. 우리가 원하는 것을 구체적으로 달성하려면 다음 매개변수를 지정해야 합니다.

- **기호** : 통화 쌍, 기본값: BTCUSD;
- **간격** : 시간적 해상도, 예를 들어 OHLC 데이터의 1분 동안 1m;
- **limit** : 반환된 데이터 포인트의 수, 기본값: 1000;
- **start** : 1970년 이후 간격의 시작 시간(밀리초);
- **end** : 1970년 이후 간격의 종료 시간(밀리초)입니다.

이제 첫 번째 쿼리를 작성하고 실행할 수 있습니다. 아래 코드는 2020년 9월 첫 달의 BTC/USD 1시간 분해능 OHLC 데이터를 반환합니다.

```python
import bitfinex
import datetime
import time
import pandas as pd

# Create api instance of the v2 API
api_v2 = bitfinex.bitfinex_v2.api_v2()

# Define query parameters
pair = 'BTCUSD' # Currency pair of interest
TIMEFRAME = '1h'#,'4h','1h','15m','1m'

# Define the start date
t_start = datetime.datetime(2018, 9, 1, 0, 0)
t_start = time.mktime(t_start.timetuple()) * 1000

# Define the end date
t_stop = datetime.datetime(2020, 10, 1, 0, 0)
t_stop = time.mktime(t_stop.timetuple()) * 1000

# Download OHCL data from API
result = api_v2.candles(symbol=pair, interval=TIMEFRAME, limit=1000, start=t_start, end=t_stop)

# Convert list of data to pandas dataframe
names = ['Date', 'Open', 'Close', 'High', 'Low', 'Volume']
df = pd.DataFrame(result, columns=names)
df['Date'] = pd.to_datetime(df['Date'], unit='ms')

# we can plot our downloaded data
import matplotlib.pyplot as plt
plt.plot(df['Open'],'-')
plt.show()
```



위의 스크립트 끝에 몇 가지 Matplotlib 라인을 추가하여 다운로드한 데이터를 플로팅할 수 있습니다. 다음과 같습니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Historical-data/example.png)

## 더 긴 시간 간격에 대한 과거 데이터

이제 우리는 1000개의 과거 데이터 단계를 다운로드하는 방법을 알고 있지만 API가 한 번에 더 많은 데이터를 다운로드할 수 없는 경우 더 많은 데이터를 다운로드하는 방법을 알고 있습니다. 따라서 예를 들어 관심 있는 시간 간격을 전체 연도로 늘리면 1시간 단위로 얻을 수 없습니다. 따라서 이 제한을 극복하려면 큰 쿼리를 여러 개의 작은 쿼리로 분할하는 함수를 작성해야 합니다. 또한 Bitfinex API에 요청할 수 있는 요청 수에는 제한이 있음을 명심해야 합니다. 현재 이 제한은 분당 60회 호출이며, 이는 각 요청 후 다음 호출을 시작하기 전에 최소 1초를 기다려야 함을 의미합니다. 아래의 기능은 안전을 위해 1.5초 동안 유지되지만 원하는 경우 변경할 수 있습니다.

```python
def fetch_data(start, stop, symbol, interval, TIMEFRAME_S):
    limit = 1000    # We want the maximum of 1000 data points
    # Create api instance
    api_v2 = bitfinex.bitfinex_v2.api_v2()
    hour = TIMEFRAME_S * 1000
    step = hour * limit
    data = []

    total_steps = (stop-start)/hour
    while total_steps > 0:
        if total_steps < limit: # recalculating ending steps
            step = total_steps * hour

        end = start + step
        data += api_v2.candles(symbol=symbol, interval=interval, limit=limit, start=start, end=end)
        print(pd.to_datetime(start, unit='ms'), pd.to_datetime(end, unit='ms'), "steps left:", total_steps)
        start = start + step
        total_steps -= limit
        time.sleep(1.5)
    return data
```



위의 기능을 사용하여 이제 더 긴 시간 간격으로 쿼리를 실행할 수 있습니다. 우리가 계산하고 제공해야 하는 유일한 추가 사항 `step`은 밀리초 단위의 크기입니다. 우리는 모든 새로운 마이너 쿼리에서 얼마나 많은 데이터 포인트를 요청해야 하는지 알기 위해 새로운 while 루프마다 단계 크기를 다시 계산합니다. 이것은 우리가 이전에 정의한 한계와 매우 유사합니다. API에 대한 요청 수를 줄이려면 최대 단계 크기로 이동하는 것이 가장 좋습니다. 1분의 경우 1단계 크기는 다음과 같이 계산됩니다.

```
시간 = 초 * 제한 * 1000ms
1분 = 60초 * 1000 * 1000 = 60000000
1시간 = 3600초 * 1000 * 1000 = 3600000000
```

마지막으로 결과를 Pandas 데이터 프레임으로 변환하려고 합니다. 그런 다음 잠재적인 중복을 제거하고 모든 것이 올바른 순서인지 확인하고 숫자 타임스탬프를 사람이 읽을 수 있는 형식으로 복원할 수 있습니다. 마지막 단계에서 다운로드한 기록 데이터를 저장하고 `.csv`파일에 저장할 수 있습니다.

```
결과 = fetch_data(t_start, t_stop, 쌍, TIMEFRAME, TIMEFRAME_S)
이름 = ['날짜', '시가', '종가', '높음', '낮음', '볼륨']
df = pd.DataFrame(결과, 열=이름)
df.drop_duplicates(inplace=True)
df['날짜'] = pd.to_datetime(df['날짜'], 단위='ms')
df.set_index('날짜', inplace=True)
df.sort_index(inplace=참)
df.to_csv(f"{쌍}_{TIMEFRAME}.csv")
```

긴 기록 데이터를 다운로드하는 전체 코드는 다음과 같습니다.

```python
import bitfinex
import datetime
import time
import pandas as pd

# Define query parameters
pair = 'BTCUSD' # Currency pair of interest
TIMEFRAME = '1h'#,'4h','1h','15m','1m'
TIMEFRAME_S = 3600 # seconds in TIMEFRAME

# Define the start date
t_start = datetime.datetime(2018, 1, 1, 0, 0)
t_start = time.mktime(t_start.timetuple()) * 1000

# Define the end date
t_stop = datetime.datetime(2020, 10, 12, 0, 0)
t_stop = time.mktime(t_stop.timetuple()) * 1000

def fetch_data(start, stop, symbol, interval, TIMEFRAME_S):
    limit = 1000    # We want the maximum of 1000 data points
    # Create api instance
    api_v2 = bitfinex.bitfinex_v2.api_v2()
    hour = TIMEFRAME_S * 1000
    step = hour * limit
    data = []

    total_steps = (stop-start)/hour
    while total_steps > 0:
        if total_steps < limit: # recalculating ending steps
            step = total_steps * hour

        end = start + step
        data += api_v2.candles(symbol=symbol, interval=interval, limit=limit, start=start, end=end)
        print(pd.to_datetime(start, unit='ms'), pd.to_datetime(end, unit='ms'), "steps left:", total_steps)
        start = start + step
        total_steps -= limit
        time.sleep(1.5)
    return data

result = fetch_data(t_start, t_stop, pair, TIMEFRAME, TIMEFRAME_S)
names = ['Date', 'Open', 'Close', 'High', 'Low', 'Volume']
df = pd.DataFrame(result, columns=names)
df.drop_duplicates(inplace=True)
df['Date'] = pd.to_datetime(df['Date'], unit='ms')
df.set_index('Date', inplace=True)
df.sort_index(inplace=True)
df.to_csv(f"{pair}_{TIMEFRAME}.csv")
```



Bitfinex API를 통해 다운로드할 수 있는 과거 데이터의 통화 쌍이 몇 개인지 궁금하다면 다음 두 줄을 실행하십시오.

```
api_v1 = bitfinex.bitfinex_v1.api_v1()
쌍 = api_v1.symbols()
```

# 여러 환경에서 모델 학습

현재 RL 비트코인 거래 봇이 직면한 중요한 문제 중 하나는 만족스러운 결과를 보기 위해 훈련하는 데 너무 오랜 시간이 걸린다는 것입니다. 저는 상당히 낮은 학습률(lr=0.00001)을 사용하고 있습니다. 더 큰 것을 사용하면서 훨씬 빠르게 학습할 수 있지만, 그러면 우리 모델이 중요한 가격 조치 기능을 학습하지 못할 수 있습니다. 그래서 어떤 면에서 우리는 학습 과정의 속도를 높여야 합니다. 내 머리에 오는 가장 좋은 방법 중 하나는 다중 처리를 사용해야 한다는 것입니다. 제 과거 튜토리얼을 따라왔다면 제 BipedalWalker-v3 Reinforcement 학습 튜토리얼에 이미 익숙할 것입니다. 여기서 여러 환경을 사용하여 에이전트를 교육했습니다. 백그라운드에서 여러 환경을 실행하고 각 환경의 상태를 감지하는 동안 많은 속도를 얻지 못했다는 것을 테스트했습니다. 그래서,

이 부분에서 나는 비슷한 일을해야한다고 결정했습니다. 백그라운드에서 여러 환경을 시작할 수 있도록 사용자 지정 환경을 재구성합니다. 이런 식으로 봇은 더 다양한 거래 조합을 시도하고 가격 조치 기능을 더 빨리 배울 수 있습니다.

## 다중 처리 예제

파이썬 멀티프로세싱에 대해 이야기하는 동안 저는 전문가는 아니지만 과거의 여러 프로젝트에서 이를 성공적으로 구현했습니다. 먼저 여러 프로세스 간에 파이프 통신을 사용하는 간단한 예를 보여 드리겠습니다. 그런 다음 사용자 지정 환경을 유사하게 구현한 방법을 보여 드리겠습니다.

우선 파이썬 멀티프로세싱이 어떻게 작동하는지 단계별로 설명하지 않겠습니다. 다중 처리 모듈을 사용하는 동안 사용자 지정 환경 프로세스 간에 데이터 공유 및 메시지 전달의 개념에 대해 설명합니다.



우리의 경우 새로 생성된 처리 환경은 다음을 수행합니다.

- 독립적으로 실행
- 자신의 기억 공간이 있습니다.

아래에서 다중 처리의 기본 사항을 이해하기 위한 예제 코드를 볼 수 있습니다. 그래서 `class Environment(Process)`백그라운드에서 실행되는 독립 프로세스를 시작하는 클래스를 만들었습니다.

단순성을 위해 결과적으로 내 프로세스는 수신된 숫자에 2를 곱한 값을 반환합니다.

단순화를 위해 이 사용자 지정 환경 내에서 두 개의 사용자 지정 프로세스만 만듭니다. 메인 프로그램과 환경 간의 통신을 위해 프로세스는 파이프 통신 방식을 사용합니다. `Pipe()`파이프의 두 끝을 나타내는 두 개의 연결 개체를 반환합니다. 각 연결 개체에는 `send()`및 `recv()`메서드가 있습니다.

```python
from multiprocessing import Process, Pipe
import time
import random

class Environment(Process): # creating environment class for multiprocessing
    def __init__(self, env_idx, child_conn):
        super(Environment, self).__init__()
        self.env_idx = env_idx
        self.child_conn = child_conn

    def run(self):
        super(Environment, self).run()
        while True:
            number = self.child_conn.recv()
            self.child_conn.send(number*2)

if __name__ == "__main__":
    works, parent_conns, child_conns = [], [], []
    
    for idx in range(2):
        parent_conn, child_conn = Pipe() # creating a communication pipe
        work = Environment(idx, child_conn) # creating new process 
        work.start() # starting process 
        works.append(work) # saving started procsses to list
        parent_conns.append(parent_conn) # saving communication pipe refference to list
        child_conns.append(child_conn) # saving communication pipe refference to list

    while True:
        for worker_id, parent_conn in enumerate(parent_conns):
            r = random.randint(0, 10) # creating random number between 0 and 10
            parent_conn.send(r) # sending message with random nuber to worker_id running process
            
        time.sleep(1)

        for worker_id, parent_conn in enumerate(parent_conns):
            result = parent_conn.recv() # reading received message from worker_id process
            print(f"From {worker_id} worker received {result}")
```



이것은 아주 간단한 코드입니다. 메인 루프에서 0에서 10 사이의 각 프로세스에 대해 난수를 생성하여 보냅니다. 내 콘솔에 스팸을 보내지 않기 위해 1초 절전 모드를 사용한 다음 다른 루프를 실행하여 답변을 위한 파이프를 읽고 이를 화면에 인쇄합니다.

## 다중 처리 환경

비슷한 원리로 우리의 다중 처리 환경은 맞춤형 비트코인 거래 봇으로 작동합니다. 간단한 숫자를 반환하는 대신 사용자 지정 거래 환경을 실행하여 예측된 작업을 수행하고 상태, 보상 및 특정 환경의 기타 매개변수를 반환합니다.

물론 내 말을 코드로 변환하는 것보다 말하기가 훨씬 쉽습니다. 여러 프로세스에서 작동하도록 스크립트를 프로그래밍하는 동안 디버그하기가 꽤 복잡하기 때문입니다. 하지만 운이 좋게도 이 까다로운 작업을 이미 수행했으므로 간단히 백그라운드에서 실행되는 독립 환경을 만드는 데 사용할 코드가 있습니다.

```python
from multiprocessing import Process, Pipe

class Environment(Process):
    def __init__(self, env_idx, child_conn, env, training_batch_size, visualize):
        super(Environment, self).__init__()
        self.env = env
        self.env_idx = env_idx
        self.child_conn = child_conn
        self.training_batch_size = training_batch_size
        self.visualize = visualize

    def run(self):
        super(Environment, self).run()
        state = self.env.reset(env_steps_size = self.training_batch_size)
        self.child_conn.send(state)
        while True:
            reset, net_worth, episode_orders = 0, 0, 0
            action = self.child_conn.recv()
            if self.env_idx == 0:
                self.env.render(self.visualize)
            state, reward, done = self.env.step(action)

            if done or self.env.current_step == self.env.end_step:
                net_worth = self.env.net_worth
                episode_orders = self.env.episode_orders
                state = self.env.reset(env_steps_size = self.training_batch_size)
                reset = 1

            self.child_conn.send([state, reward, done, reset, net_worth, episode_orders])
```



이 환경은 `[state, reward, done, reset, net_worth, episode_orders]`각 환경의 통계를 추적하고 적절한 조치를 예측하는 데 도움이 되는 매개변수를 반환합니다.

## 다중 처리 교육 에이전트

이 부분은 상당히 복잡하며, 나는 그 코드를 설명할 의무가 없다고 생각합니다. 따라서 맞춤형 Bitcoin Reinforcement Learning 거래 에이전트를 훈련할 때마다 사용할 함수를 알려 드리겠습니다. 모든 네트워크 매개변수는 이전과 동일한 방식으로 정의됩니다. 그 외에도 이제 num_worker 매개변수를 사용하여 생성하려는 작업자 수를 설정할 수 있습니다. 올바른 작업자 수를 선택하기 전에 보유하고 있는 CPU와 GPU를 고려하십시오. 훈련은 그래픽 처리 장치에서 수행되고 환경은 프로세스에서 실행되기 때문에 GPU가 있는 경우 더 많은 작업자를 실행할 수 있습니다. 그렇지 않으면 GPU가 없으면 가장 어려운 부분(트레이닝)이 CPU에서 수행되기 때문에 속도가 크게 향상되지 않습니다. 어쨌든 다음은 코드입니다.

```python
def train_multiprocessing(CustomEnv, agent, train_df, num_worker=4, training_batch_size=500, visualize=False, EPISODES=10000):
    works, parent_conns, child_conns = [], [], []
    episode = 0
    total_average = deque(maxlen=100) # save recent 100 episodes net worth
    best_average = 0 # used to track best average net worth

    for idx in range(num_worker):
        parent_conn, child_conn = Pipe()
        env = CustomEnv(train_df, lookback_window_size=agent.lookback_window_size)
        work = Environment(idx, child_conn, env, training_batch_size, visualize)
        work.start()
        works.append(work)
        parent_conns.append(parent_conn)
        child_conns.append(child_conn)

    agent.create_writer(env.initial_balance, env.normalize_value, EPISODES) # create TensorBoard writer

    states =        [[] for _ in range(num_worker)]
    next_states =   [[] for _ in range(num_worker)]
    actions =       [[] for _ in range(num_worker)]
    rewards =       [[] for _ in range(num_worker)]
    dones =         [[] for _ in range(num_worker)]
    predictions =   [[] for _ in range(num_worker)]

    state = [0 for _ in range(num_worker)]
    for worker_id, parent_conn in enumerate(parent_conns):
        state[worker_id] = parent_conn.recv()

    while episode < EPISODES:
        predictions_list = agent.Actor.actor_predict(np.reshape(state, [num_worker]+[_ for _ in state[0].shape]))
        actions_list = [np.random.choice(agent.action_space, p=i) for i in predictions_list]

        for worker_id, parent_conn in enumerate(parent_conns):
            parent_conn.send(actions_list[worker_id])
            action_onehot = np.zeros(agent.action_space.shape[0])
            action_onehot[actions_list[worker_id]] = 1
            actions[worker_id].append(action_onehot)
            predictions[worker_id].append(predictions_list[worker_id])

        for worker_id, parent_conn in enumerate(parent_conns):
            next_state, reward, done, reset, net_worth, episode_orders = parent_conn.recv()
            states[worker_id].append(np.expand_dims(state[worker_id], axis=0))
            next_states[worker_id].append(np.expand_dims(next_state, axis=0))
            rewards[worker_id].append(reward)
            dones[worker_id].append(done)
            state[worker_id] = next_state

            if reset:
                episode += 1
                a_loss, c_loss = agent.replay(states[worker_id], actions[worker_id], rewards[worker_id], predictions[worker_id], dones[worker_id], next_states[worker_id])
                total_average.append(net_worth)
                average = np.average(total_average)

                agent.writer.add_scalar('Data/average net_worth', average, episode)
                agent.writer.add_scalar('Data/episode_orders', episode_orders, episode)
                
                print("episode: {:<5} worker: {:<1} net worth: {:<7.2f} average: {:<7.2f} orders: {}".format(episode, worker_id, net_worth, average, episode_orders))
                if episode > len(total_average):
                    if best_average < average:
                        best_average = average
                        print("Saving model")
                        agent.save(score="{:.2f}".format(best_average), args=[episode, average, episode_orders, a_loss, c_loss])
                    agent.save()
                
                states[worker_id] = []
                next_states[worker_id] = []
                actions[worker_id] = []
                rewards[worker_id] = []
                dones[worker_id] = []
                predictions[worker_id] = []

    agent.end_training_log()
    # terminating processes after while loop
    works.append(work)
    for work in works:
        work.terminate()
        print('TERMINATED:', work)
        work.join()
```

좋아, 아마도 위의 기능을 사용하는 방법에 대한 예가 필요할 것입니다. 그렇죠? 이전과 크게 다르지 않습니다. 다음은 Dense RL 네트워크를 훈련시킨 방법의 예입니다.

```python
from multiprocessing_env import train_multiprocessing, test_multiprocessing
...

if __name__ == "__main__":            
    df = pd.read_csv('./BTCUSD_1h.csv')
    df = df.sort_values('Date')
    df = AddIndicators(df) # insert indicators to df

    lookback_window_size = 50
    test_window = 720*3 # 3 months 
    train_df = df[100:-test_window-lookback_window_size] # we leave 100 to have properly calculated indicators
    test_df = df[-test_window-lookback_window_size:]
    
    agent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00001, epochs=5, optimizer=Adam, batch_size = 32, model="Dense")
    train_multiprocessing(CustomEnv, agent, train_df, num_worker = 32, training_batch_size=500, visualize=False, EPISODES=200000)
```

변경할 수 있는 주요 매개변수는 lookback_window_size, learning rate(lr), epochs, optimizer, batch_size, 모델 유형, 작업자 수, training_batch_size, 교육 에피소드입니다.

이것이 많은 매개변수라고 생각한다고 가정합니다. 여기서는 가장 중요한 매개변수만 제공합니다. 이 경우 모델 최적화에 대해 이야기하기 시작할 때 훨씬 더 많은 것이 변경될 수 있지만 교육 및 테스트 결과에서 모델의 기본 보기를 생성하기에 충분한 매개변수임을 알 수 있습니다.

이전 튜토리얼을 봤다면 내가 세 가지 다른 모델(Dence, CNN, LSTM)을 만들었다는 것을 이미 알고 있을 것입니다. 그래서 저는 3가지 모두를 훈련시켰고 다음 표에서 각 모델에 대한 훈련 결과와 매개변수를 보여드릴 수 있습니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Historical-data/training_table.png)훈련 결과 표

보시다시피 저는 주로 모델 유형, 창 크기, 배치 크기를 변경하려고 했습니다. 그 결과 '베스트 모델 스코어'와 트레이닝 시간을 살펴봤다. 이 데이터에서 어떤 모델이 가장 좋은지 결정할 수 있으며 저는 CNN 네트워크에 기대고 있습니다. 물론 LSTM도 꽤 인상적이지만 훈련하는 데 2배 이상의 시간이 필요합니다.

아래는 Tensorboard에서 발췌한 것입니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Historical-data/tensorboard_training.png)

먼저 손실 곡선에 대해 간단히 설명하겠습니다.

- **Actor Loss** - Actor 손실 함수의 평균 크기입니다. 정책(조치를 결정하는 프로세스)이 얼마나 변경되고 있는지와 관련이 있습니다. **이는 성공적인 교육 세션 동안 감소해야 합니다** . 이 값은 훈련 중에 변동됩니다. 일반적으로 1.0보다 작아야 합니다.
- **Critical Loss** - Critical Loss 함수의 평균 크기입니다. 비평가 모델이 각 상태의 가치를 얼마나 잘 예측할 수 있는지와 관련이 있습니다. 크리티컬 손실은 보상이 증가할수록 증가하며, 보상이 안정되면 손실이 감소해야 합니다. 이는 에이전트가 학습하는 동안 증가해야 하며 보상이 안정화되면 감소해야 합니다.

위의 훈련 결과에서 검증 데이터로 어떤 모델이 가장 좋을지 추측하기는 꽤 어렵지만 최고의 모델 점수를 가져야 한다고 가정할 수 있습니다. 따라서 CNN 또는 LSTM 네트워크여야 합니다. 우리는 둘 다 테스트 한 후에 알게 될 것입니다.

## 다중 처리 테스트 에이전트

덜 중요한 것은 테스트 단계이므로 훈련된 모델을 테스트하기 위해 다중 처리 스크립트도 작성했습니다.

```python
def test_multiprocessing(CustomEnv, agent, test_df, num_worker = 4, visualize=False, test_episodes=1000, folder="", name="Crypto_trader", comment="", initial_balance=1000):
    agent.load(folder, name)
    works, parent_conns, child_conns = [], [], []
    average_net_worth = 0
    average_orders = 0
    no_profit_episodes = 0
    episode = 0

    for idx in range(num_worker):
        parent_conn, child_conn = Pipe()
        env = CustomEnv(test_df, initial_balance=initial_balance, lookback_window_size=agent.lookback_window_size)
        work = Environment(idx, child_conn, env, training_batch_size=0, visualize=visualize)
        work.start()
        works.append(work)
        parent_conns.append(parent_conn)
        child_conns.append(child_conn)

    state = [0 for _ in range(num_worker)]
    for worker_id, parent_conn in enumerate(parent_conns):
        state[worker_id] = parent_conn.recv()

    while episode < test_episodes:
        predictions_list = agent.Actor.actor_predict(np.reshape(state, [num_worker]+[_ for _ in state[0].shape]))
        actions_list = [np.random.choice(agent.action_space, p=i) for i in predictions_list]

        for worker_id, parent_conn in enumerate(parent_conns):
            parent_conn.send(actions_list[worker_id])

        for worker_id, parent_conn in enumerate(parent_conns):
            next_state, reward, done, reset, net_worth, episode_orders = parent_conn.recv()
            state[worker_id] = next_state

            if reset:
                episode += 1
                #print(episode, net_worth, episode_orders)
                average_net_worth += net_worth
                average_orders += episode_orders
                if net_worth < initial_balance: no_profit_episodes += 1 # calculate episode count where we had negative profit through episode
                print("episode: {:<5} worker: {:<1} net worth: {:<7.2f} average_net_worth: {:<7.2f} orders: {}".format(episode, worker_id, net_worth, average_net_worth/episode, episode_orders))
                if episode == test_episodes: break
            
    print("No profit episodes: {}".format(no_profit_episodes))
    # save test results to test_results.txt file
    with open("test_results.txt", "a+") as results:
        current_date = datetime.now().strftime('%Y-%m-%d %H:%M')
        results.write(f'{current_date}, {name}, test episodes:{test_episodes}')
        results.write(f', net worth:{average_net_worth/(episode+1)}, orders per episode:{average_orders/test_episodes}')
        results.write(f', no profit episodes:{no_profit_episodes}, comment: {comment}\n')
    
    # terminating processes after while loop
    works.append(work)
    for work in works:
        work.terminate()
        print('TERMINATED:', work)
        work.join()
```

좋아, 아마도 훈련 부분과 같을 것이다. 위 함수를 사용하는 예제가 필요하겠죠? 이전과 크게 다르지 않습니다. 다음은 Dense RL 네트워크를 훈련시킨 방법의 예입니다.

```python
from multiprocessing_env import train_multiprocessing, test_multiprocessing
...

if __name__ == "__main__":            
    df = pd.read_csv('./BTCUSD_1h.csv')
    df = df.sort_values('Date')
    df = AddIndicators(df) # insert indicators to df

    lookback_window_size = 50
    test_window = 720*3 # 3 months 
    train_df = df[100:-test_window-lookback_window_size] # we leave 100 to have properly calculated indicators
    test_df = df[-test_window-lookback_window_size:]
    
    agent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00001, epochs=5, optimizer=Adam, batch_size = 32, model="Dense")
    test_multiprocessing(CustomEnv, agent, test_df, num_worker = 16, visualize=False, test_episodes=1000, folder="2021_01_21_20_06_Crypto_trader", name="1984.93_Crypto_trader", comment="Dense")
```

일반적으로 test_df 데이터 세트에서 test_multiprocessing을 호출하고, 에피소드 테스트 횟수를 설정하고, 훈련된 모델에 대한 경로를 제공하면 됩니다. 테스트는 리소스 소모가 훨씬 적은 프로세스이므로 훈련된 모든 모델을 테스트하는 것은 문제가 되지 않습니다. 결과는 다음 표에 있습니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Historical-data/testing_table.png)

이 결과는 **보이지 않는 데이터** 에 대한 것입니다 ! 이전 튜토리얼에서 가장 중요한 매개변수는 "수익 없음 에피소드" 및 "순자산 종료"입니다. 그것들을 보면 우리 모델이 얼마나 좋은지 평가할 수 있습니다. 위 표의 테스트 데이터 세트에서 최종 순자산이 가장 높은 (CNN) 모델을 강조 표시했습니다. 3개월 동안 이 에이전트는 순자산을 48% 증가시켰고, 이 수치를 매월 평균하면 약 16.17%가 됩니다. 인상적입니다! 이전 튜토리얼에서 CNN의 성능이 좋지 않은 것을 보았지만 짧은 훈련 데이터 때문일 수 있다고 생각했습니다. 내 이론이 확인되었습니다. 더 많은 훈련 데이터가 필요했습니다.

# 결론:

따라서 고해상도 OHLC 데이터를 검색하는 것은 그렇게 복잡하지 않습니다. 무료로 다운로드할 수 있는 몇 시간 동안 구글링하는 것보다 Python 스크립트를 사용하고 다운로드하는 것이 훨씬 쉽습니다.

그래서 저는 실제로 Neural Networks로 시장을 이길 수 있다는 것을 증명했습니다. 그러나 라이브 거래를 위해 실행하는 것에 대해서는 여전히 회의적입니다. 나는 당신에게 그것을 추천하지 않습니다. 이 봇을 개선할 수 있는 부분이 아직 많이 있습니다.

지금까지 시뮬레이션의 거래는 완벽한 조건으로 이루어지고 있으므로 다음 튜토리얼에서 수행하는 모든 거래에 수수료를 추가할 것입니다. 또한 우리 모델이 더 많은 시장 기능을 학습할 수 있도록 훨씬 더 많은 지표를 추가할 수 있습니다. 또한 내 데이터 정규화 기술이 최고가 아니라고 생각합니다. 이 부분에 대한 조사를 좀 했으면 좋겠습니다.

읽어 주셔서 감사합니다! 항상 그렇듯이 이 튜토리얼에서 제공하는 모든 코드는 내 **[GitHub](https://github.com/pythonlessons/RL-Bitcoin-trading-bot/tree/main/RL-Bitcoin-trading-bot_6)** 페이지에서 찾을 수 있으며 무료로 사용할 수 있습니다!

**이 모든 튜토리얼은 교육 목적을 위한 것이며 거래 조언으로 받아들여서는 안됩니다. 투자를 잃을 가능성이 있으므로 이 튜토리얼, 이전 또는 향후 튜토리얼에서 정의된 알고리즘이나 전략을 기반으로 거래하지 않는 것이 가장 좋습니다.**





---

# 시장을 이길 수 있는 자동화된 비트코인 거래 봇에 기술 지표 통합 및 최적화! #7

## 이 튜토리얼에서는 비트코인 거래 봇을 계속 개발할 것입니다. 우리는 더 많은 기술 지표를 통합하고 정규화 기술을 구현하며 물론 모든 것을 테스트할 것입니다!

이 튜토리얼에서는 비트코인 거래 봇을 계속 개발할 것입니다. 더 많은 기술 지표를 코드에 통합하고 새로 제안된 정규화 기술을 시도하고 물론 전체 코드를 약간 수정하여 훈련된 모델을 훨씬 더 쉽게 테스트할 것입니다.

강화 학습 비트코인 거래 봇이 포함된 7번째 튜토리얼 부분에 오신 것을 환영합니다. 지금의 위치에 도달하기 위해 수천 개의 코드 라인을 작성하고 수천 시간을 모델 교육에 사용했습니다. 그러나 누군가가 현재 7번째 튜토리얼을 시작하는 데 비용이 많이 든다고 말한다면 이 튜토리얼 시리즈를 시작하게 될지 확신할 수 없습니다.



반면에 포기하지 않았다는 것이 자랑스럽고, 아무리 힘들고, 어떤 순간부터 해결책을 알지 못하더라도, 나는 내 프로젝트를 앞으로 나아갈 수 있는 힘을 찾으려고 노력했습니다. 이것은 자기 동기 부여라고 하며 나 자신을 위한 것이 아니라 내 튜토리얼 시리즈를 읽을 모든 사람을 위해 했습니다.

지금까지 우리가 한 일을 보면 나 자신이 믿기지 않습니다. 우리는 시뮬레이션 **에서 시장을 이길** 수 있는 비트코인 거래 봇을 만들었습니다 ! 지금까지 우리는 모든 속도를 높이기 위해 다중 처리를 사용하여 거래를 시뮬레이션할 수 있는 강화 학습 암호화폐 거래 환경을 만들었습니다. 또한 역사적으로 표시된 데이터를 쉽게 다운로드할 수 있는 방법을 찾았습니다. 성능을 측정하기 위해 여러 (CNN, LSTM, Dense) 신경망 아키텍처를 테스트했습니다. 이 모든 것은 우리가 한 일의 극히 일부일 뿐입니다!

지금까지 시뮬레이션의 거래는 완벽한 조건에서 이루어졌습니다. 그래서 이 튜토리얼 부분에서는 주문 수수료를 모델에 삽입하고, 더 많은 관련 없는 지표를 추가하고, 교육/테스트 데이터를 정규화하는 솔루션을 구현하고, 교육 설정을 기억하지 않고 다양한 모델을 테스트하는 솔루션을 구현하기로 결정했습니다. 따라서 이 부분은 가장 흥미롭고 흥미로운 부분 중 하나일 것입니다!

# 데이터 정규화

지금까지는 정규화 기술을 사용하지 않았습니다. 나는 모든 값을 40k로 나눴습니다. 그러나 시계열 데이터는 고정되어 있지 않다는 점을 지적하고 싶습니다(구글에서 의미하는 바를 찾을 수 있습니다). 이는 머신 러닝 모델이 훈련하는 동안 상승 추세 데이터에서 학습된 경우 하락 추세를 예측하기 어렵다는 것을 의미합니다.

차분 및 변환 기술을 사용하여 데이터를 정규 분포 형식으로 변환하여 이 문제를 해결할 수 있습니다.



종가만 표시하면 시장 데이터가 다음과 같이 표시됩니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Optimizing/Close_price.png)비트코인 종가

미분은 이전 시간 단계의 값에서 각 시간 단계의 도함수(수익률)를 빼는 과정입니다. 간단한 한 줄로 이 작업을 수행합니다.`df["Close"] = df["Close"] - df["Close"].shift(1)`

결과적으로 추세를 제거하고 다음과 같은 결과를 얻어야 합니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Optimizing/Close_Differenced_price.png)비트코인 차이 종가

결과는 꽤 흥미롭게 보이며 시각적 경향이 제거된 것 같습니다. 그러나 데이터에는 여전히 명확한 계절성이 있습니다. 데이터를 구별하기 전에 모든 시간 단계에서 로그를 취하여 이를 제거할 수 있습니다. 위의 줄과 매우 유사합니다.`df["Close"] = np.log(df["Close"]) - np.log(df["Close"].shift(1))`

이제 다음 차트를 받습니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Optimizing/Close_Logged_Differenced_price.png)비트코인 기록 차액 종가

이제 우리 인간은 우리의 비트코인 과거 데이터가 이 차트에 있다는 것을 말할 수 없다는 것이 분명합니다. 마지막 단계에서 이 데이터를 가져와서 0과 1 사이에 배치하여 이 데이터를 정규화합니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Optimizing/Close_Logged_Differenced_normalized_price.png)비트코인 기록 차등 정규화 종가

위의 네 가지 플롯을 모두 수신하는 데 사용한 몇 줄의 코드가 있습니다.

```python
if __name__ == "__main__":
    # testing normalization technieques
    df = pd.read_csv('./BTCUSD_1h.csv')
    df = df.dropna()
    df = df.sort_values('Date')

    #df["Close"] = df["Close"] - df["Close"].shift(1)
    df["Close"] = np.log(df["Close"]) - np.log(df["Close"].shift(1))

    Min = df["Close"].min()
    Max = df["Close"].max()
    df["Close"] = (df["Close"] - Min) / (Max - Min)
    
    fig = plt.figure(figsize=(16,8)) 
    plt.plot(df["Close"],'-')
    ax=plt.gca()
    ax.grid(True)
    fig.tight_layout()
    plt.show()
```

# 기술 지표

이 시리즈의 [다섯 번째 튜토리얼](https://pylessons.com/RL-BTC-BOT-indicators/) 에서 저는 시장 데이터에 지표를 삽입하는 방법을 보여주었습니다. 해당 튜토리얼에서 SMA, 볼린저 밴드, 포물선 SAR, MACD 및 RSI의 다섯 가지 지표를 삽입했습니다. 또한 해당 자습서의 끝에서 나중에 더 많이 삽입하려고 한다고 언급했으며 이 자습서는 이 작업에 적합한 시간이라고 생각합니다.

일반적으로 기술 지표는 일부 기술 분석에 사용됩니다. 그러나 배치에서 가장 사소한 상관 관계 지표만 추출하려고 하기 때문에 이것을 "기능 엔지니어링"이라고 부를 것입니다. 우리는 위에서 주어진 기술로 그것들을 정규화할 것이고, 모든 것이 우리의 강화 학습 에이전트에 공급될 것입니다.



우리가 사용할 기술 지표를 선택하기 위해 [ta 라이브러리](https://github.com/bukosabino/ta) 에 있는 모든 42개(이 튜토리얼을 작성하는 순간) 기술 지표의 상관 관계를 비교할 것 입니다. 가장 간단한 방법은 라이브러리를 사용 `pandas`하여 `seaborn`동일한 유형의 각 지표(추세, 변동성, 거래량, 모멘텀 등) 간의 상관 관계를 찾는 것입니다. 그런 다음 각 종류에서 가장 사소한 상관 관계가 있는 지표만 선택합니다. 제 생각에는 이렇게 하면 상태 크기에 너무 많은 노이즈를 추가하지 않고 가능한 한 많은 이점을 얻을 수 있습니다.

## 상관관계와 그것이 중요한 이유는 무엇입니까?

기계 학습 모델을 향상시키는 가장 빠른 방법 중 하나는 상관 관계가 높은 데이터 세트 기능을 식별하고 줄이는 것입니다. 이러한 기능은 모델에 노이즈와 부정확성을 추가하여 원하는 결과를 얻기 어렵게 만듭니다.

두 개의 독립적인 기능이 강한 관계를 가지고 있으면 양의 상관 관계 또는 음의 **상관 관계** 가 있는 것으로 간주됩니다 . 모델을 개발할 때 상관관계가 높은 변수는 출력을 왜곡할 수 있으므로 피하는 것이 좋습니다. 두 개의 독립 변수가 동일한 이벤트를 나타내는 경우 모델에 "노이즈" 또는 부정확성이 발생할 수 있습니다. 모델은 유용한 출력을 생성하기 위해 외부 정보에만 의존하며 공선(상관) 변수가 있으면 회귀 출력 중 하나 이상에서 변동이 증가할 수 있습니다. 이것은 어떤 변수가 종속 변수에 영향을 미치는지 이해하기 어렵게 하여 모델의 유용성을 평가하기 어렵게 만듭니다.

나는 왜, 어디서, 어떻게 설명하는지에 대해 깊이 설명하지 않을 것입니다. 다른 곳에도 많은 정보가 있습니다. 실용적인 내용을 이어가겠습니다.

먼저 상관된 기능을 삭제하고 시각화를 그리는 데 사용하는 함수를 간단히 설명하겠습니다.

```python
def DropCorrelatedFeatures(df, threshold, plot):
    df_copy = df.copy()

    # Remove OHCL columns
    df_drop = df_copy.drop(["Date", "Open", "High", "Low", "Close", "Volume"], axis=1)

    # Calculate Pierson correlation
    df_corr = df_drop.corr()

    columns = np.full((df_corr.shape[0],), True, dtype=bool)
    for i in range(df_corr.shape[0]):
        for j in range(i+1, df_corr.shape[0]):
            if df_corr.iloc[i,j] >= threshold or df_corr.iloc[i,j] <= -threshold:
                if columns[j]:
                    columns[j] = False
                    
    selected_columns = df_drop.columns[columns]

    df_dropped = df_drop[selected_columns]

    if plot:
        # Plot Heatmap Correlation
        fig = plt.figure(figsize=(8,8))
        ax = sns.heatmap(df_dropped.corr(), annot=True, square=True)
        ax.set_yticklabels(ax.get_yticklabels(), rotation=0) 
        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')
        fig.tight_layout()
        plt.show()
    
    return df_dropped
```



보시다시피 위의 함수를 읽는 동안 먼저 팬더의 데이터 프레임에서 OHCL 열을 제거합니다. 그들에 대한 상관 관계를 계산할 필요는 없습니다. 다음으로 상관 관계를 계산하는 것은 코드 한 줄을 작성하는 것만큼 간단합니다. `df_corr = df_drop.corr()`. 그게 다야, 이제 임계값을 초과하는 지표를 삭제해야 합니다. 상관 관계를 계산하는 것과 비슷하고 간단한 것이 있어야 하지만 찾을 수 없어 for 루프를 사용하여 수행했습니다. 그리고 마지막으로 아름다운 히트맵 시각화를 위한 코드 라인을 작성했습니다. 이것은 이 튜토리얼을 위해 특별히 수행되었습니다. 이 함수를 사용하여 각 표시기 유형의 히트맵을 플로팅합니다.

## 추세 지표

ta 라이브러리에서 기본 지표의 가장 중요한 부분은 추세 지표입니다. 총 14개의 주어진 지표가 있습니다. **추세 지표는 물론 추세** 가 있는 경우 시장이 어떤 방향으로 움직이고 있는지 알려줍니다  . 나는 이 지표들 각각의 이름을 밝히지 않을 것이며 이것은 나와 당신의 시간 낭비가 될 것입니다. 따라서 아래는 추세 표시기의 히트맵을 가져오는 데 사용할 함수입니다.

```python
def get_trend_indicators(df, threshold=0.5, plot=False):
    df_trend = df.copy()
    
    # add custom trend indicators
    df_trend["sma7"] = SMAIndicator(close=df["Close"], window=7, fillna=True).sma_indicator()
    df_trend["sma25"] = SMAIndicator(close=df["Close"], window=25, fillna=True).sma_indicator()
    df_trend["sma99"] = SMAIndicator(close=df["Close"], window=99, fillna=True).sma_indicator()

    df_trend = add_trend_ta(df_trend, high="High", low="Low", close="Close")

    return DropCorrelatedFeatures(df_trend, threshold, plot)
```

보시다시피 여기에 3개의 사용자 지정 표시기를 추가했습니다 . `sma7`, `sma25`및 `sma99`; 마찬가지로 지표를 추가할 수 있습니다. 그래서 아마 지금 이 기능을 어떻게 사용하는지 궁금해 하실 것입니다. 그렇죠? 더 간단할 수 없습니다. 우리는 다음을 수행합니다.



```
df = pd.read_csv('./BTCUSD_1h.csv')
df = df.sort_values('날짜')
get_trend_indicators(df, 임계값=0.5, 플롯=True)
```

시각화를 위해 임계값 0.5와 플롯을 True로 사용하기로 선택했습니다. 위의 코드를 실행하면 결과적으로 다음 히트맵을 얻을 수 있습니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Optimizing/trend_indicators.png)

히트맵을 계산하기 전에는 14+3개의 지표가 있었습니다. 총 17개의 지표였습니다. 상관 관계를 삭제한 후 7개만 남게 되었습니다. 이는 모든 추세 지표의 59%가 상관 관계가 있음을 의미합니다.

## 변동성 지표

두 번째 지표 배치는 5개의 변동성 지표입니다. 이는 자산이 평균 방향 값에서 얼마나 벗어났는지를 측정하는 고유한 기술 지표입니다. 이것은 복잡하게 들릴지 모르지만 매우 간단합니다. 자산의 **변동성** 이 높을 때 평균 방향에서 멀어집니다 . 예를 들어, 지진은 예상 기상 조건에 비해 변동성이 높습니다. 추세 지표와 매우 유사하게 변동성 지표의 히트맵을 얻는 데 사용할 함수를 만들었습니다.

```python
def get_volatility_indicators(df, threshold=0.5, plot=False):
    df_volatility = df.copy()
    
    # add custom volatility indicators
    # ...

    df_volatility = add_volatility_ta(df_volatility, high="High", low="Low", close="Close")

    return DropCorrelatedFeatures(df_volatility, threshold, plot)
```



이전과 같은 방식으로 위의 함수를 다음과 같은 방식으로 실행해야 합니다.



```
df = pd.read_csv('./BTCUSD_1h.csv')
df = df.sort_values('날짜')
get_volatility_indicators(df, 임계값=0.5, 플롯=True)
```

Matplotlib는 다음과 같은 결과를 제공해야 합니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Optimizing/volatility_indicators.png)

변동성 히트맵을 계산하기 전에 다섯 가지 지표가 있었습니다. 결과적으로 정확한 지표 수가 표시됩니다. 이는 우수한 결과이며, 이는 모든 지표가 서로 관련되지 않은 특성을 제공하는 서로 다른 기술을 사용하여 계산된다는 것을 의미합니다. 이는 모든 변동성 지표 중 어느 것도 상관관계가 없었음을 의미합니다.

## 볼륨 표시기

세 번째 배치는 잘 알려진 볼륨 표시기입니다. 볼륨은 주어진 기간 동안 유가 증권이 거래 된 주식 수를 보여줍니다. **거래량 지표** 는 가장 일반적으로 사용되는 거래 플랫폼에서 시각적으로 표현되는 간단한 수학 공식입니다. 이전과 같은 방식으로 볼륨 표시기의 히트맵을 가져오는 데 사용할 함수를 만들었습니다.

```python
def get_volume_indicators(df, threshold=0.5, plot=False):
    df_volume = df.copy()
    
    # add custom volume indicators
    # ...

    df_volume = add_volume_ta(df_volume, high="High", low="Low", close="Close", volume="Volume")

    return DropCorrelatedFeatures(df_volume, threshold, plot)
```

위의 함수를 다음과 같은 방식으로 실행합니다.

```
df = pd.read_csv('./BTCUSD_1h.csv')
df = df.sort_values('날짜')
get_volume_indicators(df, 임계값=0.5, 플롯=True)
```

Matplotlib는 다음과 같은 결과를 제공해야 합니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Optimizing/volume_indicators.png)

히트맵을 계산하기 전에는 9개의 지표가 있었습니다. 상관 관계를 삭제한 후 7개가 남았습니다. 이는 모든 거래량 지표의 22%만이 상관관계가 있음을 의미합니다. 이는 인상적인 결과입니다. 지금도 임계값을 줄일 수 있음을 알 수 있지만 지표의 수는 상관 관계가 매우 낮기 때문에 그대로 유지됩니다.

## 모멘텀 지표

모멘텀 지표는 시간 경과에 따른 가격의 움직임과 가격이 움직이는 방향에 관계없이 그러한 움직임이 얼마나 강력했는지, 현재 또는 앞으로도 그럴 것인지를 보여줍니다. 모멘텀 지표는 트레이더와 분석가가 시장이 역전될 수 있는 지점을 인식하는 데 도움이 되기 때문에 도움이 된다고 합니다. 다음 기능을 사용하여 이러한 표시기를 추가합니다.

```python
def get_momentum_indicators(df, threshold=0.5, plot=False):
    df_momentum = df.copy()
    
    # add custom momentum indicators
    # ...

    df_momentum = add_momentum_ta(df_momentum, high="High", low="Low", close="Close", volume="Volume")

    return DropCorrelatedFeatures(df_momentum, threshold, plot)
```

위의 함수를 다음과 같은 방식으로 실행합니다.

```
df = pd.read_csv('./BTCUSD_1h.csv')
df = df.sort_values('날짜')
get_momentum_indicators(df, 임계값=0.5, 플롯=True)
```

Matplotlib는 다음과 같은 결과를 제공해야 합니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Optimizing/momentum_indicators.png)

히트맵을 계산하기 전에 11개의 지표가 있었습니다. 상관 관계를 삭제한 후 4개만 남게 되었습니다. 이는 모든 모멘텀 지표의 64%가 상관관계가 있음을 의미합니다. 이는 모멘텀 지표가 가장 상관관계가 있음을 의미합니다.

## 기타 지표

지표의 마지막 배치는 일일 수익률(DR), 일일 로그 수익률(DLR), 누적 수익률(CR)입니다. 그것들이 무엇인지 잘 모르겠지만 어쨌든 다음 기능을 사용하여 모델에 추가해서는 안 되는 이유를 모르겠습니다.

```python
def get_others_indicators(df, threshold=0.5, plot=False):
    df_others = df.copy()
    
    # add custom indicators
    # ...

    df_others = add_others_ta(df_others, close="Close")

    return DropCorrelatedFeatures(df_others, threshold, plot)
```

위의 함수를 다음과 같은 방식으로 실행합니다.

```
df = pd.read_csv('./BTCUSD_1h.csv')
df = df.sort_values('날짜')
get_others_indicators(df, 임계값=0.5, 플롯=True)
```

Matplotlib는 다음과 같은 결과를 제공해야 합니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Optimizing/others_indicators.png)

지표가 3개밖에 없었고 그 중 하나가 상관관계가 높았기 때문에 이에 대해 별로 할 말이 없습니다. 모든 지표를 다른 그룹으로 분리하지 않고 상관 관계를 계산하려는 경우 다음 함수를 실행할 수 있습니다.

```python
def get_all_indicators(df, threshold=0.5, plot=False):
    df_all = df.copy()
    
    # add custom indicators
    # ...

    df_all = add_all_ta_features(df_all, open="Open", high="High", low="Low", close="Close", volume="Volume")

    return DropCorrelatedFeatures(df_all, threshold, plot)
```

모든 것이 이전과 완전히 동일합니다.

따라서 이제 각 지표 그룹에서 상관 관계를 계산하고 삭제하는 데 필요한 모든 함수가 있으면 다음 함수를 실행해야 합니다.

```python
def indicators_dataframe(df, threshold=0.5, plot=False):
    trend       = get_trend_indicators(df, threshold=threshold, plot=plot)
    volatility  = get_volatility_indicators(df, threshold=threshold, plot=plot)
    volume      = get_volume_indicators(df, threshold=threshold, plot=plot)
    momentum    = get_momentum_indicators(df, threshold=threshold, plot=plot)
    others      = get_others_indicators(df, threshold=threshold, plot=plot)
    #all_ind = get_all_indicators(df, threshold=threshold)

    final_df = [df, trend, volatility, volume, momentum, others]
    result = pd.concat(final_df, axis=1)

    return result
```

위의 기능은 모든 지표를 하나의 아름다운 팬더의 데이터 프레임으로 병합하고 RL Bitcoin 거래 봇 모델에 추가로 사용할 것입니다.

진행하기 전에 정상화로 돌아가고 싶습니다. 처음에 언급했듯이 지표를 정규화하는 데 사용할 또 다른 정규화 방법을 제공했습니다. 그러나 여기에 또 다른 문제가 있습니다. 지표가 음수 값을 가질 수 있으며 이러한 음수 값에 대수를 적용하려고 하면 NaN 결과를 얻습니다. 그래서 다음 함수에서 결과 값이 음수이면 로그를 사용하지 않기로 결정했습니다.

```python
def Normalizing(df_original):
    df = df_original.copy()
    column_names = df.columns.tolist()
    for column in column_names[1:]:
        # Logging and Differencing
        test = np.log(df[column]) - np.log(df[column].shift(1))
        if test[1:].isnull().any():
            df[column] = df[column] - df[column].shift(1)
        else:
            df[column] = np.log(df[column]) - np.log(df[column].shift(1))
        # Min Max Scaler implemented
        Min = df[column].min()
        Max = df[column].max()
        df[column] = (df[column] - Min) / (Max - Min)

    return df
```

# 교육 및 테스트

데이터 세트에 지표를 삽입하는 방법과 데이터 세트를 정규화하는 방법을 변경했기 때문에 작은 코드를 변경한 곳이 많이 있었습니다. 이 자습서는 변경 사항에 대해서만 다루기 때문에 이 자습서에서 이러한 변경 사항을 언급하지 않기로 결정했습니다. 그러나 가장 광범위한 변경 사항을 소개하겠습니다.

이전의 모든 튜토리얼 시리즈 파트에서 모델을 훈련하기 시작했을 때 모든 매개변수가 `parameters.txt`파일에 작성되었습니다. 나는 단순히 txt로 작성하는 대신 JSON 구조로 작성하기로 결정했습니다.

```python
def start_training_log(self, initial_balance, normalize_value, train_episodes):      
    # save training parameters to Parameters.json file for future
    current_date = datetime.now().strftime('%Y-%m-%d %H:%M')
    params = {
        "training start": current_date,
        "initial balance": initial_balance,
        "training episodes": train_episodes,
        "lookback window size": self.lookback_window_size,
        "depth": self.depth,
        "lr": self.lr,
        "epochs": self.epochs,
        "batch size": self.batch_size,
        "normalize value": normalize_value,
        "model": self.model,
        "comment": self.comment,
        "saving time": "",
        "Actor name": "",
        "Critic name": "",
    }
    with open(self.log_name+"/Parameters.json", "w") as write_file:
        json.dump(params, write_file, indent=4)
```

JSON 구조를 사용하는 동안 과거에 훈련한 특정 모델을 테스트하려는 경우 훈련 설정을 로드하는 것이 훨씬 쉽습니다. 그래서, 이 JSON 개선은 저를 변화 `test_agent`와 `test_multiprocessing`기능으로 이끌었습니다. 이제 더 적은 수의 매개변수로 호출하기만 하면 됩니다.

이전 튜토리얼에서 거래를 하는 동안 주문 수수료를 고려하지 않는다는 많은 의견을 받았습니다. 나는 이것을 마지막으로 추가하기에 좋은 곳이라고 결정했습니다. 그래서 내 단계 기능을 약간 변경했습니다.

```python
def step(self, action):
    ...
    if action == 0: # Hold
        pass

    elif action == 1 and self.balance > self.initial_balance*0.05:
        # Buy with 100% of current balance
        self.crypto_bought = self.balance / current_price
        self.crypto_bought *= (1-self.fees) # substract fees
        self.balance -= self.crypto_bought * current_price
        self.crypto_held += self.crypto_bought
        self.trades.append({'Date' : Date, 'High' : High, 'Low' : Low, 'total': self.crypto_bought, 'type': "buy", 'current_price': current_price})
        self.episode_orders += 1

    elif action == 2 and self.crypto_held*current_price> self.initial_balance*0.05:
        # Sell 100% of current crypto held
        self.crypto_sold = self.crypto_held
        self.crypto_sold *= (1-self.fees) # substract fees
        self.balance += self.crypto_sold * current_price
        self.crypto_held -= self.crypto_sold
        self.trades.append({'Date' : Date, 'High' : High, 'Low' : Low, 'total': self.crypto_sold, 'type': "sell", 'current_price': current_price})
        self.episode_orders += 1
```

두 개의 새로운 줄이 있는 것을 볼 수 있습니다.

- `self.crypto_bought *= (1-self.fees)`우리가 행동을 살 때;
- `self.crypto_sold *= (1-self.fees)`우리가 행동을 팔 때.

두 가지 조치 모두 암호화폐든 현금이든 잔액에서 수수료를 뺍니다. 대부분의 거래소에서 이 수수료는 약 0.1%이지만, 필요한 경우 `self.fees`코드에서 줄을 검색하여 원하는 대로 변경할 수 있습니다.

`punish_value`또한 비트코인을 끝까지 보유하는 대신 봇이 거래를 하도록 동기를 부여하는 데 사용된 와 관련된 모든 항목을 제거했음을 알 수 있습니다 .

이제 많은 지표를 사용하기 때문에 데이터 준비가 훨씬 더 복잡합니다. 아래 예에서 볼 수 있습니다.

```python
if __name__ == "__main__":            
    df = pd.read_csv('./BTCUSD_1h.csv')
    df = df.dropna()
    df = df.sort_values('Date')

    #df = AddIndicators(df) # insert indicators to df
    df = indicators_dataframe(df, threshold=0.5, plot=False) # insert indicators to df
    depth = len(list(df.columns[1:])) # OHCL + indicators without Date

    df_nomalized = Normalizing(df[99:])[1:].dropna() # we cut first 100 bars to have properly calculated indicators
    df = df[100:].dropna() # we cut first 100 bars to have properly calculated indicators

    lookback_window_size = 100
    test_window = 720*3 # 3 months
    
    # split training and testing datasets
    train_df = df[:-test_window-lookback_window_size]
    test_df = df[-test_window-lookback_window_size:]
    
    # split training and testing normalized datasets
    train_df_nomalized = df_nomalized[:-test_window-lookback_window_size]
    test_df_nomalized = df_nomalized[-test_window-lookback_window_size:]

    # multiprocessing training/testing. Note - run from cmd or terminal
    agent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00001, epochs=5, optimizer=Adam, batch_size=32, model="CNN", depth=depth, comment="Normalized")
    train_multiprocessing(CustomEnv, agent, train_df, train_df_nomalized, num_worker = 32, training_batch_size=500, visualize=False, EPISODES=400000)
    
    test_multiprocessing(CustomEnv, CustomAgent, test_df, test_df_nomalized, num_worker = 16, visualize=False, test_episodes=1000, folder="2021_02_11_15_40_Crypto_trader", name="", comment="")
AddIndicators`이제 함수 를 호출하는 대신 `indicators_dataframe`현재 예제에서 30개의 다른 표시기를 추가하는 함수를 실행합니다. 나는 이것을 우리가 모델 `indicators_count`의 권리를 구성하는 데 사용하는 깊이 라고 부릅니다.`state_size
```

이제 데이터 프레임 이 있음 `train_df`을 알 수 있습니다. 정규화되지 않음 — 주로 모델이 훈련되는 동안 아름다운 시각화를 렌더링하고 보상을 계산할 때만 사용됩니다. 훈련용으로만 사용하는 정규화된 데이터입니다.`train_df_normalized``Train_df``Train_df_normalized`

`CustomAgent`매개변수가 있는 클래스가 있는 에이전트를 만들고 모든 것을 `train_multiprocessing`함수에 공급하여 교육 프로세스를 시작해야 합니다.

 더 많은 지표와 새로운 정규화 기술로 인한 성능 향상을 비교하기 위해 [이전 자습서](https://pylessons.com/RL-BTC-BOT-Historical-data/) 에서 사용한 것과 동일한 매개변수로 에이전트를 훈련했습니다 . 다음은 모델 `Parameters.json`의 모든 지표가 포함된 파일입니다 .`2021_02_18_21_48_Crypto_trader`

```python
{
 “training start”: “2021–02–18 21:48”,
 “initial balance”: 1000,
 “training episodes”: 400000,
 “lookback window size”: 100,
 “depth”: 30,
 “lr”: 1e-05,
 “epochs”: 5,
 “batch size”: 32,
 “normalize value”: 40000,
 “model”: “CNN”,
 “comment”: “Normalized, no punish value”,
 “saving time”: “2021–02–21 05:09”,
 “Actor name”: “3906.52_Crypto_trader_Actor.h5”,
 “Critic name”: “3906.52_Crypto_trader_Critic.h5”
}
```

좋아요; 약 55시간이 소요되는 400,000번의 훈련 단계에 대해 모델을 훈련했습니다. 최고의 모델이 명명 `3906.52_Crypto_trader`되었고 3개월 동안의 테스트 결과는 다음과 같습니다. 결국 1162.08$, 예상보다 좋지 않습니다. 매월 보상, 에피소드 주문 및 무수익 에피소드를 표시하는 차트를 만들었습니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Optimizing/3906.52_Crypto_trader.png)

보시다시피 첫 달에는 수익을 냈지만 두 달 뒤에는 그리 크지 않았습니다. 하락 추세가 있더라도 봇이 이를 처리할 수 있기를 바랍니다.

그런 다음 이전 자습서에서와 같이 함수 `AddIndicators`대신 사용하여 다른 모델을 훈련하기로 결정했습니다. `indicators_dataframe`이전 튜토리얼과의 유일한 차이점은 이제 새로운 정규화 기술을 사용한다는 것입니다. 다음은 `2021_02_21_17_54_Crypto_trader`모델의 모든 지표가 포함된 Parameters.json 파일입니다.

```python
{
 “training start”: “2021–02–21 17:54”,
 “initial balance”: 1000,
 “training episodes”: 400000,
 “lookback window size”: 100,
 “depth”: 13,
 “lr”: 1e-05,
 “epochs”: 5,
 “batch size”: 32,
 “normalize value”: 40000,
 “model”: “CNN”,
 “comment”: “Normalized, no punish value”,
 “saving time”: “2021–02–23 11:44”,
 “Actor name”: “3263.63_Crypto_trader_Actor.h5”,
 “Critic name”: “3263.63_Crypto_trader_Critic.h5”
}
```

보시다시피 이제 깊이는 30이 아닌 13이며 훈련 시간(42시간)이 덜 소요되었습니다. 3개월의 동일한 테스트 데이터 세트 내에서 테스트 결과가 끝날 때 1478.50$의 순자산이 있었습니다. 이것은 [이전 튜토리얼](https://pylessons.com/RL-BTC-BOT-Historical-data/) 과 매우 유사합니다 . 다음은 이전과 유사한 3개월 차트입니다.

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Optimizing/3263.63_Crypto_trader.png)

우리 모델에 너무 많은 지표가 추가되면 교육 데이터에 노이즈가 발생하여 우리 모델이 시장 가격 행동을 학습할 수 없다고 가정합니다. 이것이 우리 봇이 많은 것보다 적은 지표로 더 잘 수행하는 이유입니다. 다음은 동일한 교육 및 테스트 데이터 세트를 사용하는 [이전 자습서](https://pylessons.com/RL-BTC-BOT-Historical-data/) 모델과 유사한 비교입니다 .

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Optimizing/3142.13_Crypto_trader%20(previous%20tutorial).png)

보시다시피, 이전 모델은 테스트 기간 동안 수익 에피소드가 더 적었고 총 순자산도 조금 더 좋았기 때문에 약간 더 나은 성능을 보였습니다.

어쨌든 지표가 적은 현재 모델이 더 나은 성능을 보인다고 생각합니다. 데이터에서 추세를 제거하기 위해 새로운 정규화 기술을 사용했기 때문에 우리 모델은 비트코인만 거래하는 방법을 배우지 않습니다. 우리 모델은 우리가 테스트해야 하는 다른 시장 쌍을 수익성 있게 거래할 수도 있지만 이것은 이 튜토리얼의 주제가 아닙니다.

`2021_02_21_17_54_Crypto_trader`보이지 않는 데이터를 거래하는 동안 봇은 다음과 같이 보입니다 .

![img](https://pylessons.com/media/Tutorials/RL-BTC-BOT/RL-BTC-BOT-Optimizing/gameplay.gif)

상승 추세가 있거나 시장이 횡보할 때 실적이 매우 인상적입니다. 나는 그것이 딥을 피하는 법을 배웠기 때문에 기쁩니다. 하지만 단기 추세가 하락할 때는 그다지 좋지 않지만 장기적으로 볼 때 비트코인이 대부분 상승 방향으로만 움직이기 때문일 수 있습니다.

# 결론:

우리는 이 튜토리얼에서 더 많은 기술적 지표가 항상 RL Bitcoin 거래 봇의 더 나은 성능을 제공한다는 것을 의미하지 않는다는 것을 배웠습니다. 이는 대부분의 지표가 지연되어 훈련 데이터 세트에 너무 많은 노이즈를 추가하기 때문입니다.

첫 번째 튜토리얼에서 무작위 거래 에이전트를 개발하여 달성한 더 큰 그림은 이제 인상적인 결과를 얻었습니다. 공개적으로 말하면 이 튜토리얼을 하는 동안 시장을 이기고 이런 종류의 자동화 봇이 거래를 수익성 있게 만드는 것이 가능하리라고는 기대조차 하지 않았습니다.

물론 이 봇은 완벽하지 않습니다. 숙련된 자동화된 봇 코더는 개선할 위치와 방법에 대해 많은 조언을 줄 수 있습니다. 그래도 여가 시간에 직접 개발하고 연구했기 때문에 결과는 기대 이상이었습니다!

이 튜토리얼 코드는 원하는 곳 어디에서나 복사하여 사용할 수 있지만 실제 거래에는 사용하지 않는 것이 좋습니다. 돈을 모두 잃을 수 있기 때문입니다. 그렇게 하면 저를 탓하지 마세요! 어쨌든 강화 학습이 포함된 이 모든 작업에는 너무 많은 노력, 시간 및 지식이 필요했습니다. 이것은 이것이 나의 마지막 비트코인 거래 튜토리얼 부분이며 곧 다시 돌아오지 않을 수도 있음을 의미합니다. 절대 안 된다고 말하지는 않지만 더 나은 결과를 얻으려면 여전히 더 많은 강화 학습과 더 심오한 기계 학습 지식이 필요합니다!

지금은 다른 Python 및 기계 학습 영역을 계속 연구할 것입니다. 당신이 관심이 있다면, 당신은 나를 따라갈 수 있습니다.

읽어 주셔서 감사합니다! 항상 그렇듯이 이 튜토리얼에서 제공하는 모든 코드는 내 [GitHub](https://github.com/pythonlessons/RL-Bitcoin-trading-bot/tree/main/RL-Bitcoin-trading-bot_7) 페이지에서 찾을 수 있으며 무료로 사용할 수 있습니다!

**이 모든 튜토리얼은 교육 목적을 위한 것이며 거래 조언으로 받아들여서는 안됩니다. 투자를 잃을 가능성이 있으므로 이 튜토리얼, 이전 또는 향후 튜토리얼에서 정의된 알고리즘이나 전략을 기반으로 거래하지 않는 것이 가장 좋습니다.**

